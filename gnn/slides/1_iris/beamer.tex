\documentclass{beamer}
\usepackage{xeCJK}
\usepackage[T1]{fontenc}
\usepackage{ifplatform}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{chemformula}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{fancyvrb}
\usepackage{etoolbox}

\defaultfontfeatures{Mapping=tex-text, Scale=MatchLowercase}
\setmainfont{Times}
\setmonofont{Times}

% Avoid ^I in minted output
\setminted{tabsize=4}



% macOS font setting
\ifmacosx
	\setCJKmainfont{Songti SC}
	\setCJKmonofont{Songti SC}
\else
% Linux and other OS font setting
	\setCJKmainfont{Source Han Sans CN}
	\setCJKmonofont{Source Han Sans CN}
\fi

% Theme
% \usetheme{metropolis}

% Information to be included in the title page:
\title{模型介绍}
\author{吴清柳}
\institute{Beijing University of Posts and Telecommunications}
\date{\today}

\begin{document}
\AtBeginSection{\tableofcontents[currentsection]}
% Title page
\maketitle

% Table of contents
\begin{frame}
	\frametitle{Table of Contentes}
	\tableofcontents
\end{frame}

\section{概述}
\begin{frame}
	\frametitle{Overview}
	通过该ppt, 对9个实验使用的模型进行介绍. 九个实验分别是鸢尾花分类实验，emojify人
	脸表情识别实验, 贷款预测, 房价预测, MNIST手写数字识别, 股票价格预测, 泰坦尼克号
	生还预测, 红酒质量预测和假新闻预测.

	使用的模型有机器学习方法如决策树, 逻辑回归, XGBoost等; 以及深度学习方法如全连接
	网络, CNN等.
\end{frame}

\section{鸢尾花分类实验}
\begin{frame}
	\frametitle{鸢尾花分类实验}
	\begin{block}{代码和数据集}
		该论文的复现代码在\href{这里}{https://github.com/Micuks/code/blob/master/gnn/1\_iris/iris-classification/train.ipynb}
		数据集使用了鸢尾花分类数据集.
	\end{block}

	\begin{block}{模型信息}
		模型使用PyTorch编写, 为三层的全连接网络. 前两层使用ReLU激活函数, 第三层为完成分
		类功能, 使用Softmax作为激活函数, 输出分类结果.
		\begin{itemize}
			\item 第一层全连接层输入特征数量为4, 输出特征数量为25;
			\item 第二层全连接层输入特征数量为25, 输出特征数量为30;
			\item 第三层全连接层输入特征数量为30, 输出特征数量为3, 对应三类鸢尾花;
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{模型信息}
	模型搭建代码如下.
	\begin{minted}[breaklines]{python}
class Model(nn.Module):
def __init__(self, input_feats=4, hidden_layer1=25, hidden_layer2=30, output_feats=3) -> None:
	super().__init__()
	self.fc1 = nn.Linear(input_feats, hidden_layer1)
	self.fc2=nn.Linear(hidden_layer1, hidden_layer2)
	self.out=nn.Linear(hidden_layer2, output_feats)

def forward(self, x):
	x=F.relu(self.fc1(x))
	x=F.relu(self.fc2(x))
	x=self.out(x)

	return x
	\end{minted}
\end{frame}

\subsection{模型介绍}
\begin{frame}
	\frametitle{模型介绍}

	输入数据为鸢尾花(Iris)的特征数据. 鸢尾花有三种, 分别是Setosa, Versicolour,
	Virginica. 每个都有4个特征: sepal length, sepal width, 以及petal width.

	模型的结构介绍如下.
	\begin{block}{模型结构}
		\begin{enumerate}
			\item \textbf{输入层} 神经元数量为4, 代表4个输入特征.
			\item \textbf{第一个隐藏层`fc1'} 全连接层(dense layer), 25个神经元, 激活函数为
			      ReLU, 当输入为正时输出输入本身, 否则输出0. 如果没有非线性激活函数, 则模型将等效
			      为线性拟合函数, 拟合性能下降.
			\item \textbf{第二个隐藏层`fc2'} 另一个全连接层, 30个神经元, 激活函数为ReLU;
			\item \textbf{输出层`out'} 一个全连接层, 3个神经元, 对应鸢尾花的三个分类:
			      Setosa, Versicolour, Virginica.
		\end{enumerate}
	\end{block}

\end{frame}

\subsection{训练方法}
\begin{frame}[fragile]
	\frametitle{训练方法}
	使用交叉熵作为损失函数, Adam为优化器, 学习率为0.01, 在训练集上进行100轮训练. 对loss可视化观察后看到大约40轮后模型已经接近收敛.
	\begin{block}{训练代码}
		\begin{minted}[breaklines=true]{python}
epochs=100
losses=[]
for i in range(epochs):
    y_pred=model.forward(X_train)
    loss=criterion(y_pred,y_train)
    losses.append(loss)
    print(f'epoch: {i:2} loss: {loss.item():10.8f}')
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
	\end{minted}
	\end{block}
\end{frame}

% Slide 1: Introduction to Training Duration and Loss Storage
\begin{frame}[fragile]
	\frametitle{初始化}
	\begin{itemize}
		\item 一轮代表对所有训练样本的一个完全的前向传播和反向传播过程;
		\item \texttt{losses}列表会存储在每轮中计算的loss值, 用于追踪模型的表现;
	\end{itemize}
	\begin{minted}[breaklines]{python}
		epochs = 100
		losses = []
		\end{minted}
\end{frame}

% Slide 2: The Training Loop - Forward Pass and Loss Calculation
\begin{frame}[fragile]
	\frametitle{训练循环: 前向传播和损失计算}
	Within the loop, the model predicts outcomes and computes the discrepancy between predictions and actual values.
	在训练循环中, 模型预测输出, 计算预测值和实际值之间的差异;

	\begin{minted}[breaklines]{python}
for i in range(epochs):
    y_pred = model.forward(X_train)
    loss = criterion(y_pred, y_train)
    losses.append(loss)
\end{minted}
\end{frame}

% Slide 3: The Training Loop - Logging, Backward Pass, and Optimization
\begin{frame}[fragile]
	\frametitle{训练循环: 日志和优化}
	\begin{itemize}
		\item 通过打印轮数和loss值来监控训练进程;
		\item 计算梯度并更新模型参数来降低loss;
	\end{itemize}

	\begin{minted}[breaklines]{python}
    print(f'epoch: {i:2} loss: {loss.item():10.8f}')
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
\end{minted}
\end{frame}

\section{Emojify表情识别实验}
\begin{frame}
	\frametitle{利用CNN进行面部表情识别}

	在该实验中, 利用Tensorflow来搭建CNN网络进行对摄像头采集的人脸进行表情识别. 模型使用
	GPU进行训练, 得到的权重文件作为表情识别的依据.

	此处使用Tensorflow是为了将Pytorch和Tensorflow两个常用框架都进行学习.

\end{frame}

\subsection{模型介绍}
% Slide 1: Introduction to the Model
\begin{frame}[fragile]
	\frametitle{CNN模型结构}
	\begin{itemize}
		\item 目的: 从灰度图片进行表情识别;
		\item 图片输入形状: \(48 \times 48 \times 1\);
		\item 层之间顺序堆叠;
	\end{itemize}
	\begin{minted}[breaklines]{python}
emotion_model = Sequential()
\end{minted}
	Keras中的`Sequential'是层之间的线性堆叠. 可以允许我们来通过将层逐个堆叠来搭建神经网络.
\end{frame}

% Slide 2: Initial Convolution Layers
\begin{frame}[fragile]
	\frametitle{初始化卷积层}
	\begin{minted}[breaklines]{python}
emotion_model.add(
    Conv2D(32, kernel_size=(3, 3), activation="relu", input_shape=(48, 48, 1))
)
emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation="relu"))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Dropout(0.25))
\end{minted}
	\only<1>{\begin{block}{第一个卷积层}
			第一个卷积层有32个卷积核, 每个尺寸为$3\times 3$, 来处理尺寸为$48\times 48$的输
			入图片. `relu'激活函数引入了非线性, 让模型可以捕获复杂的特征.
		\end{block}}
	\only<2>{\begin{block}{第二个卷积层}
			第二个卷积层有64个$3\times 3$的卷积核, 该层进一步处理了来自前一层的特征图, 检测了输入图片中的更复杂的特征.
		\end{block}}
	\only<3>{\begin{block}{Max-Pooling层}
			该层通过从每个$2\times 2$窗口中取最大值来对特征图进行降采样. 这减少了计算负担, 并帮助让模型平移不变.
		\end{block}}
	\only<4>{\begin{block}{Dropout层}
			Dropout可以用来避免过拟合. 0.25的Dropout率意味着大约25\%的神经元在前面的层会被随
			机在训练中关闭, 使得可以获得容错率更高的模型.
		\end{block}}
\end{frame}

% Slide 3: Deeper Convolution Layers
\begin{frame}[fragile]
	\frametitle{多个卷积层}
	\begin{minted}[breaklines]{python}
emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation="relu"))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation="relu"))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Dropout(0.25))
\end{minted}
	\only<1>{\begin{block}{更深的卷积层}
			后面的具有128个卷积核的卷积层每个都进一步细化了特征图. 随着模型变深, 这些层会捕获图片中更多的高层特征.

			每个卷积层后面都有一个Max-Pooling层来降采样, 降低计算负担, 提高平移不变性.
		\end{block}}
\end{frame}

% Slide 4: Fully Connected Layers
\begin{frame}[fragile]
	\frametitle{全连接层}
	\begin{minted}[breaklines]{python}
emotion_model.add(Flatten())
emotion_model.add(Dense(1024, activation="relu"))
emotion_model.add(Dropout(0.25))
emotion_model.add(Dense(7, activation="softmax"))
\end{minted}

	\only<1>{\begin{block}{Flatten扁平化层}
			该层将2D的特征矩阵映射到1D的向量. 在将数据传入全连接层之前, 这是必要的步骤;
		\end{block}}
	\only<2>{\begin{block}{稠密全连接层}
			具有1024个神经元的全连接层允许模型来基于由卷积层提取的高层次特征来进行决策. `ReLU'激活函数确保了非线性.
		\end{block}}
	\only<3>{\begin{block}{输出层}
			输出稠密层有7个神经元, 对应在数据集中的七种表情分类. `softmax'激活函数确保了输出值是和为1的概率.
		\end{block}}
	\only<4>{\begin{block}{Softmax}
			Softmax函数常常被用在基于神经网络的分类器的最后一层, 将一个实数向量转化为一个概
			率分布.

			在数学上, 对一个向量$z$的Softmax函数$\sigma$可以被定义为
			\begin{equation}
				\sigma(z)_i=\frac{e^{z_i}}{\sum^K_{j=1}{e^{z_j}}}
			\end{equation}
			其中,
			\begin{itemize}
				\item $\sigma(z)_i$是$z$的第$i^{th}$个分量的Softmax函数输出;
				\item $K$是分类的数量(也是向量$z$的长度);
			\end{itemize}
		\end{block}}
	\only<5>{\begin{block}{Softmax}
			指数函数确保了输出向量所有的分量都是非负的, 除法将所有的分量规范化, 让他们的和为
			1. 结果向量因此可以表示$K$个类别的概率分布.
		\end{block}}
	\only<6>{\begin{block}{为什么使用Softmax}
			\begin{enumerate}
				\item \textbf{概率解释} softmax函数被用于分类模型的输出层, 因为他提供了多个类之
				      间的概率分布. 这意味着对于给定的输入, 模型可以提供输入可能属于哪个输出的类别;
				\item \textbf{处理多个类 }在分类问题中, 尤其是对于超过两个类别的情况, 分类时需要
				      不仅知道哪个类是最可能属于的, 还想预测和其他类的似然成都. Softmax可以通过将每个
				      类的输出压缩在0和1之间, 并确保他们的和为1来帮助实现这一目标.
				\item \textbf{基于梯度的优化} Softmax与类别交叉熵作为loss函数, 提供了一个表现良
				      好的梯度. 在训练神经网络的反向传播阶段非常有用. 没有良好的梯度, 神经网络将不能高效的学习.
			\end{enumerate}
		\end{block}}
\end{frame}

\begin{frame}[fragile]{Cross-Entropy}
	Cross-entropy是衡量两个概率分布之间的差异的方法. 在机器学习和深度学习中, 它常常
	被用作损失函数来量化预测概率分布和真实分布之间的差异.

	对两个离散的概率分布$p$和$q$, 交叉熵$H(p,q)$被定义为
	\begin{equation}
		H(p,q)=-\sum_x{p(x)\log{(q(x))}}
	\end{equation}
	其中,
	\begin{itemize}
		\item $p(x)$是事件$x$发生的真实概率;
		\item $q(x)$是事件$x$发生的预测概率;
		\item 对于所有的可能事件$x$进行求和;
	\end{itemize}

	对于用于分类任务的神经网络, $p$是真实标签的独热编码向量, $q$是预测概率(经常通过
	softmax函数取得).
\end{frame}

\begin{frame}
	\frametitle{为什么使用Cross-entropy}

	\begin{enumerate}
		\item \textbf{概率解释 } Cross-entropy提供了真实概率分布和预测概率分布之间的差异
		      的度量. 较低的交叉熵值表明模型的预测与真实标签比较相似.
		\item \textbf{可微分 } 对于学习算法, 尤其是神经网络, 具有一个可微分的损失函数是
		      至关重要的. 交叉熵可微分, 使得他可以被用于像梯度下降这样的基于梯度的优化方法.
		\item \textbf{惩罚自信的错误预测 } 交叉熵作为loss函数的一个优点是它如何处理预测.
		      如果一个模型对一个错误的类别预测0.99的概率, 对正确的类别预测0.01的概率, 交叉熵会
		      变得非常大. 这个特征确保了当模型非常自信得犯错的时候, 会被严重惩罚;
		\item \textbf{和Softmax配合良好} 交叉熵损失结合输出层中的softmax激活函数是分类问
		      题中流行的组合. 从这个组合中获得的梯度有良好的属性, 使得模型可以更快收敛, 并且经
		      常相较于其他的激活函数-损失函数对具有更好的解决方案.
		\item \textbf{解决饱和问题 } 在神经网络中, sigmoid激活函数和quadratic cost可以导
		      致``饱和''问题, 此时神经元的输出和梯度都几乎是0, 使得网络难以训练. 交叉熵不会受
		      这个问题困扰, 使得训练更加高效.
	\end{enumerate}

\end{frame}

\subsection{模型训练}
\begin{frame}[fragile]
	\frametitle{模型训练}

	神经网络模型的结构确定后, 下一个关键步骤就是在数据上对其进行训练.

	\begin{enumerate}
		\item<1-> \textbf{编译模型}
			\only<1>{
				\begin{itemize}
					\item \textbf{损失函数 } 模型使用`categorical\_crossentropy'作为损失函数.
					\item \textbf{优化器 } 使用`Adam'作为优化器. Adam对每个参数调节学习率. 其具有高
					      效率与低内存需求. `learning\_rate'战术决定了优化器最小化loss的步长, `decay'可以
					      随时间降低学习率来允许在之后的训练过程中更细化的权重更新.
					\item \textbf{矩阵 } `accuracy'是在训练过程中监控的矩阵. 准确率提供了一个清晰的,
					      直观的对模型执行情况的理解: 表示正确分类的实例在总实例中占的百分比.
				\end{itemize}}
		\item<2-> \textbf{拟合模型 }
			\only<2>{
				\begin{itemize}
					\item \textbf{数据源 } 模型借助`fit\_generator'方法进行训练, 该方法允许边读取边
					      数据增强(data augmentation on-the-fly), 且更加内存高效. 该方法当训练集太大而无法
					      整个放入内存的时候更合适.
					\item \textbf{轮数 } 模型训练了50轮. 每轮代表一次对于所有训练样本的前向传播和反
					      向传播.
					\item \textbf{Batch Size } 每一轮的步数(`28709 // 64')表示数据每次被传入大小为64
					      个实例的批次. 类似的, 对于验证阶段, 使用批次大小`7178 // 64'. 使用批次加速了训练
					      过程, 因为模型权重的更新是在每一轮后进行的, 而不是在每个数据点之后.
					\item \textbf{验证数据 }模型的表现在一个单独的验证数据几上并行验证. 这帮助了监控
					      任何的过拟合信号, 当模型在训练数据上表现异常良好, 而对于新的没有见过的数据非常糟糕的时候就发生了过拟合.
				\end{itemize}}
	\end{enumerate}

\end{frame}

\section{贷款预测}
\begin{frame}
	\frametitle{使用逻辑回归模型的贷款预测}

	借助scikit-learn的逻辑回归模型对是否可以给用户贷款进行预测. 该实验主要内容在于数
	据预处理.

\end{frame}

\subsection{模型训练和预测}
\begin{frame}[fragile]{模型训练和预测}
	\begin{minted}[breaklines,tabsize=4]{python}
LR=LogisticRegression()
LR.fit(X_train,y_train)
y_hat=LR.predict(X_test)

# prediction summary by species
print(classification_report(y_test,y_hat))

# accuracy score
LR_SC=accuracy_score(y_hat,y_test)
print('accuracy is',accuracy_score(y_hat,y_test))
\end{minted}
\end{frame}

\begin{frame}[fragile]{逻辑回归}
	`Logistic Regression'是一个用来进行二元分类问题的统计方法, 一个分类问题指预测一
	个特定数据属于两个中的哪一个类. 被叫做``logistic'' 回归因为是基于逻辑函数的, 这
	被用于建模将一个给定的输入点划入两个分类之一的概率.
\end{frame}

\begin{frame}[fragile]{训练模型}
	\begin{minted}[breaklines,tabsize=4]{python}
LR=LogisticRegression()
LR.fit(X_train,y_train)
\end{minted}
	\begin{itemize}
		\item 首先, 借助`LogisticRegression()'来获取一个逻辑回归模型.
		\item 下一步, `fit'方法将模型在训练数据上进行训练. 这里, `X\_train'是具有特征的
		      训练数据, `y\_train'是训练数据对应的标签.
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{进行预测}
	\begin{minted}[breaklines,tabsize=4]{python}
y_hat=LR.predict(X_test)
	\end{minted}
	在将模型在训练数据上训练后, 下一步就是在新的没有见过的模型上进行预测. `predict'
	方法将测试数据`X\_test'作为参数, 返回预测标签`y\_hat'.
\end{frame}

\begin{frame}[fragile]{分类报告}
	\begin{minted}[breaklines,tabsize=4]{python}
	print(classification_report(y_test,y_hat))
	\end{minted}
	`classification\_report'提供了模型表现的准确度, 召回度和F1-score.
	\begin{itemize}
		\item \textbf{Precision准确度}
			      在所有预测为某个特定类的例子中, 多少是真阳性;
		      
		\item \textbf{Recall召回率}
			      在所有属于一个特定类的实例中, 哪些被正确预测了, 即真阳性与真阴性的和;
		      
		\item \textbf{F1-score}
			      准确度和召回率的调和平均值, 提供了两个之间的平衡指标.
		      
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Precision准确度}
	\begin{equation}
		\text{Precision}=\frac{\text{True Positives}}{\text{True Positives}+\text{False Positives}}
	\end{equation}

	当假阳性的代价高的时候, 准确度是关键的.
\end{frame}

\begin{frame}[fragile]{Recall召回率}
	\begin{equation}
		\text{Recall}=\frac{\text{True Positives}}{\text{True Positives}+\text{False Negatives}}
	\end{equation}

	当假阴性的代价高的时候, 召回率是重要的.
\end{frame}

\begin{frame}[fragile]{准确率分数}
	\begin{minted}[breaklines,tabsize=4]{python}
LR_SC=accuracy_score(y_hat,y_test)
print('accuracy is',accuracy_score(y_hat,y_test))
	\end{minted}

	准确率分数是一个可以表明在测试数据中正确预测的分类的占比的量. 计算方式为将正确预
	测的数量除以预测的总数.
	\begin{equation}
		\text{Accuracy}=\frac{\text{Number of correct predictions}}{\text{Total number of predictions}}
	\end{equation}

	对于本处的贷款预测:
	\begin{itemize}
		\item 如果一个贷款请求倾向于被允许, 模型会预测`1', 否则为0.
		\item 准确率通过将预测结果与实际情况对比告诉我们多少的贷款请求被我们的模型正确预测.
	\end{itemize}

\end{frame}
\subsection{模型局限}
\begin{frame}[fragile]{模型局限}
	逻辑回归具有这样的局限:
	\begin{description}
		\item[线性决策边界] 逻辑回归假设一个线性决策边界, 这可能不能捕获数据中的复杂关系.
		\item[特征的独立性] 逻辑回归假设输入特征是独立的, 意味着他们不会互相影响. 如果一
			些特征是强相关的, 这可能影响模型的参数.
	\end{description}
\end{frame}

\end{document}
