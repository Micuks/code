\documentclass{beamer}
\usepackage{xeCJK}
\usepackage[T1]{fontenc}
\usepackage{ifplatform}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{chemformula}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{fancyvrb}
\usepackage{etoolbox}

\defaultfontfeatures{Mapping=tex-text, Scale=MatchLowercase}
\setmainfont{Times}
\setmonofont{Times}

% Avoid ^I in minted output
\setminted{tabsize=4}



% macOS font setting
\ifmacosx
	\setCJKmainfont{Songti SC}
	\setCJKmonofont{Songti SC}
\else
% Linux and other OS font setting
	\setCJKmainfont{Source Han Sans CN}
	\setCJKmonofont{Source Han Sans CN}
\fi

% Theme
% \usetheme{metropolis}

% Information to be included in the title page:
\title{模型介绍}
\author{吴清柳}
\institute{Beijing University of Posts and Telecommunications}
\date{\today}

\begin{document}
\AtBeginSection{\tableofcontents[currentsection]}
% Title page
\maketitle

% Table of contents
\begin{frame}
	\frametitle{Table of Contentes}
	\tableofcontents
\end{frame}

\section{概述}
\begin{frame}
	\frametitle{Overview}
	通过该ppt, 对9个实验使用的模型进行介绍. 九个实验分别是鸢尾花分类实验，emojify人
	脸表情识别实验, 贷款预测, 房价预测, MNIST手写数字识别, 股票价格预测, 泰坦尼克号
	生还预测, 红酒质量预测和假新闻预测.

	使用的模型有机器学习方法如决策树, 逻辑回归, XGBoost等; 以及深度学习方法如全连接
	网络, CNN等.
\end{frame}

\section{鸢尾花分类实验}
\begin{frame}
	\frametitle{鸢尾花分类实验}
	\begin{block}{代码和数据集}
		该论文的复现代码在\href{这里}{https://github.com/Micuks/code/blob/master/gnn/1\_iris/iris-classification/train.ipynb}
		数据集使用了鸢尾花分类数据集.
	\end{block}

	\begin{block}{模型信息}
		模型使用PyTorch编写, 为三层的全连接网络. 前两层使用ReLU激活函数, 第三层为完成分
		类功能, 使用Softmax作为激活函数, 输出分类结果.
		\begin{itemize}
			\item 第一层全连接层输入特征数量为4, 输出特征数量为25;
			\item 第二层全连接层输入特征数量为25, 输出特征数量为30;
			\item 第三层全连接层输入特征数量为30, 输出特征数量为3, 对应三类鸢尾花;
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}[fragile]
	\frametitle{模型信息}
	模型搭建代码如下.
	\begin{minted}[breaklines]{python}
class Model(nn.Module):
def __init__(self, input_feats=4, hidden_layer1=25, hidden_layer2=30, output_feats=3) -> None:
	super().__init__()
	self.fc1 = nn.Linear(input_feats, hidden_layer1)
	self.fc2=nn.Linear(hidden_layer1, hidden_layer2)
	self.out=nn.Linear(hidden_layer2, output_feats)

def forward(self, x):
	x=F.relu(self.fc1(x))
	x=F.relu(self.fc2(x))
	x=self.out(x)

	return x
	\end{minted}
\end{frame}

\subsection{模型介绍}
\begin{frame}
	\frametitle{模型介绍}

	输入数据为鸢尾花(Iris)的特征数据. 鸢尾花有三种, 分别是Setosa, Versicolour,
	Virginica. 每个都有4个特征: sepal length, sepal width, 以及petal width.

	模型的结构介绍如下.
	\begin{block}{模型结构}
		\begin{enumerate}
			\item \textbf{输入层} 神经元数量为4, 代表4个输入特征.
			\item \textbf{第一个隐藏层`fc1'} 全连接层(dense layer), 25个神经元, 激活函数为
			      ReLU, 当输入为正时输出输入本身, 否则输出0. 如果没有非线性激活函数, 则模型将等效
			      为线性拟合函数, 拟合性能下降.
			\item \textbf{第二个隐藏层`fc2'} 另一个全连接层, 30个神经元, 激活函数为ReLU;
			\item \textbf{输出层`out'} 一个全连接层, 3个神经元, 对应鸢尾花的三个分类:
			      Setosa, Versicolour, Virginica.
		\end{enumerate}
	\end{block}

\end{frame}

\subsection{训练方法}
\begin{frame}[fragile]
	\frametitle{训练方法}
	使用交叉熵作为损失函数, Adam为优化器, 学习率为0.01, 在训练集上进行100轮训练. 对loss可视化观察后看到大约40轮后模型已经接近收敛.
	\begin{block}{训练代码}
		\begin{minted}[breaklines=true]{python}
epochs=100
losses=[]
for i in range(epochs):
    y_pred=model.forward(X_train)
    loss=criterion(y_pred,y_train)
    losses.append(loss)
    print(f'epoch: {i:2} loss: {loss.item():10.8f}')
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
	\end{minted}
	\end{block}
\end{frame}

% Slide 1: Introduction to Training Duration and Loss Storage
\begin{frame}[fragile]
	\frametitle{初始化}
	\begin{itemize}
		\item 一轮代表对所有训练样本的一个完全的前向传播和反向传播过程;
		\item \texttt{losses}列表会存储在每轮中计算的loss值, 用于追踪模型的表现;
	\end{itemize}
	\begin{minted}[breaklines]{python}
		epochs = 100
		losses = []
		\end{minted}
\end{frame}

% Slide 2: The Training Loop - Forward Pass and Loss Calculation
\begin{frame}[fragile]
	\frametitle{训练循环: 前向传播和损失计算}
	Within the loop, the model predicts outcomes and computes the discrepancy between predictions and actual values.
	在训练循环中, 模型预测输出, 计算预测值和实际值之间的差异;

	\begin{minted}[breaklines]{python}
for i in range(epochs):
    y_pred = model.forward(X_train)
    loss = criterion(y_pred, y_train)
    losses.append(loss)
\end{minted}
\end{frame}

% Slide 3: The Training Loop - Logging, Backward Pass, and Optimization
\begin{frame}[fragile]
	\frametitle{训练循环: 日志和优化}
	\begin{itemize}
		\item 通过打印轮数和loss值来监控训练进程;
		\item 计算梯度并更新模型参数来降低loss;
	\end{itemize}

	\begin{minted}[breaklines]{python}
    print(f'epoch: {i:2} loss: {loss.item():10.8f}')
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
\end{minted}
\end{frame}

\section{Emojify表情识别实验}
\begin{frame}
	\frametitle{利用CNN进行面部表情识别}

	在该实验中, 利用Tensorflow来搭建CNN网络进行对摄像头采集的人脸进行表情识别. 模型使用
	GPU进行训练, 得到的权重文件作为表情识别的依据.

	此处使用Tensorflow是为了将Pytorch和Tensorflow两个常用框架都进行学习.

\end{frame}

\subsection{模型介绍}
% Slide 1: Introduction to the Model
\begin{frame}[fragile]
	\frametitle{CNN模型结构}
	\begin{itemize}
		\item 目的: 从灰度图片进行表情识别;
		\item 图片输入形状: \(48 \times 48 \times 1\);
		\item 层之间顺序堆叠;
	\end{itemize}
	\begin{minted}[breaklines]{python}
emotion_model = Sequential()
\end{minted}
	Keras中的`Sequential'是层之间的线性堆叠. 可以允许我们来通过将层逐个堆叠来搭建神经网络.
\end{frame}

% Slide 2: Initial Convolution Layers
\begin{frame}[fragile]
	\frametitle{初始化卷积层}
	\begin{minted}[breaklines]{python}
emotion_model.add(
    Conv2D(32, kernel_size=(3, 3), activation="relu", input_shape=(48, 48, 1))
)
emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation="relu"))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Dropout(0.25))
\end{minted}
	\only<1>{\begin{block}{第一个卷积层}
			第一个卷积层有32个卷积核, 每个尺寸为$3\times 3$, 来处理尺寸为$48\times 48$的输
			入图片. `relu'激活函数引入了非线性, 让模型可以捕获复杂的特征.
		\end{block}}
	\only<2>{\begin{block}{第二个卷积层}
			第二个卷积层有64个$3\times 3$的卷积核, 该层进一步处理了来自前一层的特征图, 检测了输入图片中的更复杂的特征.
		\end{block}}
	\only<3>{\begin{block}{Max-Pooling层}
			该层通过从每个$2\times 2$窗口中取最大值来对特征图进行降采样. 这减少了计算负担, 并帮助让模型平移不变.
		\end{block}}
	\only<4>{\begin{block}{Dropout层}
			Dropout可以用来避免过拟合. 0.25的Dropout率意味着大约25\%的神经元在前面的层会被随
			机在训练中关闭, 使得可以获得容错率更高的模型.
		\end{block}}
\end{frame}

% Slide 3: Deeper Convolution Layers
\begin{frame}[fragile]
	\frametitle{多个卷积层}
	\begin{minted}[breaklines]{python}
emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation="relu"))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation="relu"))
emotion_model.add(MaxPooling2D(pool_size=(2, 2)))
emotion_model.add(Dropout(0.25))
\end{minted}
	\only<1>{\begin{block}{更深的卷积层}
			后面的具有128个卷积核的卷积层每个都进一步细化了特征图. 随着模型变深, 这些层会捕获图片中更多的高层特征.

			每个卷积层后面都有一个Max-Pooling层来降采样, 降低计算负担, 提高平移不变性.
		\end{block}}
\end{frame}

% Slide 4: Fully Connected Layers
\begin{frame}[fragile]
	\frametitle{全连接层}
	\begin{minted}[breaklines]{python}
emotion_model.add(Flatten())
emotion_model.add(Dense(1024, activation="relu"))
emotion_model.add(Dropout(0.25))
emotion_model.add(Dense(7, activation="softmax"))
\end{minted}

	\only<1>{\begin{block}{Flatten扁平化层}
			该层将2D的特征矩阵映射到1D的向量. 在将数据传入全连接层之前, 这是必要的步骤;
		\end{block}}
	\only<2>{\begin{block}{稠密全连接层}
			具有1024个神经元的全连接层允许模型来基于由卷积层提取的高层次特征来进行决策. `ReLU'激活函数确保了非线性.
		\end{block}}
	\only<3>{\begin{block}{输出层}
			输出稠密层有7个神经元, 对应在数据集中的七种表情分类. `softmax'激活函数确保了输出值是和为1的概率.
		\end{block}}
	\only<4>{\begin{block}{Softmax}
			Softmax函数常常被用在基于神经网络的分类器的最后一层, 将一个实数向量转化为一个概
			率分布.

			在数学上, 对一个向量$z$的Softmax函数$\sigma$可以被定义为
			\begin{equation}
				\sigma(z)_i=\frac{e^{z_i}}{\sum^K_{j=1}{e^{z_j}}}
			\end{equation}
			其中,
			\begin{itemize}
				\item $\sigma(z)_i$是$z$的第$i^{th}$个分量的Softmax函数输出;
				\item $K$是分类的数量(也是向量$z$的长度);
			\end{itemize}
		\end{block}}
	\only<5>{\begin{block}{Softmax}
			指数函数确保了输出向量所有的分量都是非负的, 除法将所有的分量规范化, 让他们的和为
			1. 结果向量因此可以表示$K$个类别的概率分布.
		\end{block}}
	\only<6>{\begin{block}{为什么使用Softmax}
			\begin{enumerate}
				\item \textbf{概率解释} softmax函数被用于分类模型的输出层, 因为他提供了多个类之
				      间的概率分布. 这意味着对于给定的输入, 模型可以提供输入可能属于哪个输出的类别;
				\item \textbf{处理多个类 }在分类问题中, 尤其是对于超过两个类别的情况, 分类时需要
				      不仅知道哪个类是最可能属于的, 还想预测和其他类的似然成都. Softmax可以通过将每个
				      类的输出压缩在0和1之间, 并确保他们的和为1来帮助实现这一目标.
				\item \textbf{基于梯度的优化} Softmax与类别交叉熵作为loss函数, 提供了一个表现良
				      好的梯度. 在训练神经网络的反向传播阶段非常有用. 没有良好的梯度, 神经网络将不能高效的学习.
			\end{enumerate}
		\end{block}}
\end{frame}

\begin{frame}[fragile]{Cross-Entropy}
	Cross-entropy是衡量两个概率分布之间的差异的方法. 在机器学习和深度学习中, 它常常
	被用作损失函数来量化预测概率分布和真实分布之间的差异.

	对两个离散的概率分布$p$和$q$, 交叉熵$H(p,q)$被定义为
	\begin{equation}
		H(p,q)=-\sum_x{p(x)\log{(q(x))}}
	\end{equation}
	其中,
	\begin{itemize}
		\item $p(x)$是事件$x$发生的真实概率;
		\item $q(x)$是事件$x$发生的预测概率;
		\item 对于所有的可能事件$x$进行求和;
	\end{itemize}

	对于用于分类任务的神经网络, $p$是真实标签的独热编码向量, $q$是预测概率(经常通过
	softmax函数取得).
\end{frame}

\begin{frame}
	\frametitle{为什么使用Cross-entropy}

	\begin{enumerate}
		\item \textbf{概率解释 } Cross-entropy提供了真实概率分布和预测概率分布之间的差异
		      的度量. 较低的交叉熵值表明模型的预测与真实标签比较相似.
		\item \textbf{可微分 } 对于学习算法, 尤其是神经网络, 具有一个可微分的损失函数是
		      至关重要的. 交叉熵可微分, 使得他可以被用于像梯度下降这样的基于梯度的优化方法.
		\item \textbf{惩罚自信的错误预测 } 交叉熵作为loss函数的一个优点是它如何处理预测.
		      如果一个模型对一个错误的类别预测0.99的概率, 对正确的类别预测0.01的概率, 交叉熵会
		      变得非常大. 这个特征确保了当模型非常自信得犯错的时候, 会被严重惩罚;
		\item \textbf{和Softmax配合良好} 交叉熵损失结合输出层中的softmax激活函数是分类问
		      题中流行的组合. 从这个组合中获得的梯度有良好的属性, 使得模型可以更快收敛, 并且经
		      常相较于其他的激活函数-损失函数对具有更好的解决方案.
		\item \textbf{解决饱和问题 } 在神经网络中, sigmoid激活函数和quadratic cost可以导
		      致``饱和''问题, 此时神经元的输出和梯度都几乎是0, 使得网络难以训练. 交叉熵不会受
		      这个问题困扰, 使得训练更加高效.
	\end{enumerate}

\end{frame}

\subsection{模型训练}
\begin{frame}[fragile]
	\frametitle{模型训练}

	神经网络模型的结构确定后, 下一个关键步骤就是在数据上对其进行训练.

	\begin{enumerate}
		\item<1-> \textbf{编译模型}
			\only<1>{
				\begin{itemize}
					\item \textbf{损失函数 } 模型使用`categorical\_crossentropy'作为损失函数.
					\item \textbf{优化器 } 使用`Adam'作为优化器. Adam对每个参数调节学习率. 其具有高
					      效率与低内存需求. `learning\_rate'战术决定了优化器最小化loss的步长, `decay'可以
					      随时间降低学习率来允许在之后的训练过程中更细化的权重更新.
					\item \textbf{矩阵 } `accuracy'是在训练过程中监控的矩阵. 准确率提供了一个清晰的,
					      直观的对模型执行情况的理解: 表示正确分类的实例在总实例中占的百分比.
				\end{itemize}}
		\item<2-> \textbf{拟合模型 }
			\only<2>{
				\begin{itemize}
					\item \textbf{数据源 } 模型借助`fit\_generator'方法进行训练, 该方法允许边读取边
					      数据增强(data augmentation on-the-fly), 且更加内存高效. 该方法当训练集太大而无法
					      整个放入内存的时候更合适.
					\item \textbf{轮数 } 模型训练了50轮. 每轮代表一次对于所有训练样本的前向传播和反
					      向传播.
					\item \textbf{Batch Size } 每一轮的步数(`28709 // 64')表示数据每次被传入大小为64
					      个实例的批次. 类似的, 对于验证阶段, 使用批次大小`7178 // 64'. 使用批次加速了训练
					      过程, 因为模型权重的更新是在每一轮后进行的, 而不是在每个数据点之后.
					\item \textbf{验证数据 }模型的表现在一个单独的验证数据几上并行验证. 这帮助了监控
					      任何的过拟合信号, 当模型在训练数据上表现异常良好, 而对于新的没有见过的数据非常糟糕的时候就发生了过拟合.
				\end{itemize}}
	\end{enumerate}

\end{frame}

\section{贷款预测}
\begin{frame}
	\frametitle{使用逻辑回归模型的贷款预测}

	借助scikit-learn的逻辑回归模型对是否可以给用户贷款进行预测. 该实验主要内容在于数
	据预处理.

\end{frame}

\subsection{模型训练和预测}
\begin{frame}[fragile]{模型训练和预测}
	\begin{minted}[breaklines,tabsize=4]{python}
LR=LogisticRegression()
LR.fit(X_train,y_train)
y_hat=LR.predict(X_test)

# prediction summary by species
print(classification_report(y_test,y_hat))

# accuracy score
LR_SC=accuracy_score(y_hat,y_test)
print('accuracy is',accuracy_score(y_hat,y_test))
\end{minted}
\end{frame}

\begin{frame}[fragile]{逻辑回归}
	`Logistic Regression'是一个用来进行二元分类问题的统计方法, 一个分类问题指预测一
	个特定数据属于两个中的哪一个类. 被叫做``logistic'' 回归因为是基于逻辑函数的, 这
	被用于建模将一个给定的输入点划入两个分类之一的概率.
\end{frame}

\begin{frame}[fragile]{训练模型}
	\begin{minted}[breaklines,tabsize=4]{python}
LR=LogisticRegression()
LR.fit(X_train,y_train)
\end{minted}
	\begin{itemize}
		\item 首先, 借助`LogisticRegression()'来获取一个逻辑回归模型.
		\item 下一步, `fit'方法将模型在训练数据上进行训练. 这里, `X\_train'是具有特征的
		      训练数据, `y\_train'是训练数据对应的标签.
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{进行预测}
	\begin{minted}[breaklines,tabsize=4]{python}
y_hat=LR.predict(X_test)
	\end{minted}
	在将模型在训练数据上训练后, 下一步就是在新的没有见过的模型上进行预测. `predict'
	方法将测试数据`X\_test'作为参数, 返回预测标签`y\_hat'.
\end{frame}

\begin{frame}[fragile]{分类报告}
	\begin{minted}[breaklines,tabsize=4]{python}
	print(classification_report(y_test,y_hat))
	\end{minted}
	`classification\_report'提供了模型表现的准确度, 召回度和F1-score.
	\begin{itemize}
		\item \textbf{Precision准确度}
		      在所有预测为某个特定类的例子中, 多少是真阳性;

		\item \textbf{Recall召回率}
		      在所有属于一个特定类的实例中, 哪些被正确预测了, 即真阳性与真阴性的和;

		\item \textbf{F1-score}
		      准确度和召回率的调和平均值, 提供了两个之间的平衡指标.

	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Precision准确度}
	\begin{equation}
		\text{Precision}=\frac{\text{True Positives}}{\text{True Positives}+\text{False Positives}}
	\end{equation}

	当假阳性的代价高的时候, 准确度是关键的.
\end{frame}

\begin{frame}[fragile]{Recall召回率}
	\begin{equation}
		\text{Recall}=\frac{\text{True Positives}}{\text{True Positives}+\text{False Negatives}}
	\end{equation}

	当假阴性的代价高的时候, 召回率是重要的.
\end{frame}

\begin{frame}[fragile]{准确率分数}
	\begin{minted}[breaklines,tabsize=4]{python}
LR_SC=accuracy_score(y_hat,y_test)
print('accuracy is',accuracy_score(y_hat,y_test))
	\end{minted}

	准确率分数是一个可以表明在测试数据中正确预测的分类的占比的量. 计算方式为将正确预
	测的数量除以预测的总数.
	\begin{equation}
		\text{Accuracy}=\frac{\text{Number of correct predictions}}{\text{Total number of predictions}}
	\end{equation}

	对于本处的贷款预测:
	\begin{itemize}
		\item 如果一个贷款请求倾向于被允许, 模型会预测`1', 否则为0.
		\item 准确率通过将预测结果与实际情况对比告诉我们多少的贷款请求被我们的模型正确预测.
	\end{itemize}

\end{frame}
\subsection{模型局限}
\begin{frame}[fragile]{模型局限}
	逻辑回归具有这样的局限:
	\begin{description}
		\item[线性决策边界] 逻辑回归假设一个线性决策边界, 这可能不能捕获数据中的复杂关系.
		\item[特征的独立性] 逻辑回归假设输入特征是独立的, 意味着他们不会互相影响. 如果一
			些特征是强相关的, 这可能影响模型的参数.
	\end{description}
\end{frame}

\section{房价预测}
\begin{frame}[fragile]{在Boston Hosting数据集上进行房价预测}
	借助PyTorch搭建全连接模型对Boston Hosting数据集进行房价预测.
\end{frame}

\subsection{模型结构}
\begin{frame}[fragile]{模型结构}
	\begin{minted}[breaklines,tabsize=4]{python}
	# boston housing model
class BostonHousingModel(nn.Module):
    def __init__(self):
        super(BostonHousingModel,self).__init__()
        self.layer1=nn.Linear(13,64)
        self.layer2=nn.Linear(64,64)
        self.layer3=nn.Linear(64,1)
        
    def forward(self,x):
        x=torch.relu(self.layer1(x))
        x=torch.relu(self.layer2(x))
        x=self.layer3(x)
        return x
	\end{minted}
	模型借助PyTorch框架, 使用`nn.Module'基类来定义神经网络结构.
\end{frame}

\begin{frame}[fragile]{模型初始化}
	\only<1>{\begin{block}{第一层}
			全连接层, 输入特征为13, 对应Boston Housing dataset中的13个特征, 输出特征数量为64.
		\end{block}}
	\only<1>{\begin{block}{第二层}
			全连接层, 输入特征数量为64, 输出特征数量为64.
		\end{block}}
	\only<1>{\begin{block}{第三层}
			全连接层, 输入特征数量为64, 输出特征数量为1, 即表示预测房价结果.
		\end{block}}
\end{frame}

\begin{frame}[fragile]{前向传播}
	前向传播方法描述了输入数据是如何流过模型的.
	\begin{itemize}
		\item \textbf{第一步 }将输入`x'通过`layer1', 应用ReLU(Rectified Linear Unit)激活
		      函数. ReLu函数引入非线性, 允许模型从错误中学习和调整, 对于学习复杂的特征至关重
		      要;
		\item \textbf{第二步 }将前一步的输出传入`layer2', 再次使用ReLU激活函数;
		\item \textbf{最后一步 }将结果传过`layer3'. 在这一层没有激活函数, 因为这是一个回
		      归问题, 模型被设计为直接输出连续值.
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{模型训练}
	\begin{itemize}
		\item \textbf{损失函数 } 使用`nn.MSELoss()'作为损失函数. 平均平方差损失计算了预
		      测和实际值的平均平方差, 对于房价预测问题是合适的选择.
		\item \textbf{优化器选择 } 选择Adam优化器来调节模型参数. 学习率为`0.001'.
		\item \textbf{训练轮数 } 训练500轮, 并记录日志.
	\end{itemize}
\end{frame}

\subsection{改进模型}

\begin{frame}[fragile]{改进模型}
	在前面基础模型的基础上, 使用Dropout进行改进. 在训练策略上, 使用学习率规划下降和提前停止来优化训练.
	\begin{minted}[breaklines,tabsize=4]{python}
class OptiBostonHousingModel(nn.Module):
    def __init__(self):
        super(OptiBostonHousingModel,self).__init__()
        self.layer1=nn.Linear(13,128)
        self.layer2=nn.Linear(128,64)
        self.dropout=nn.Dropout(0.5)
        self.layer3=nn.Linear(64,1)
        
    def forward(self,x):
        x=torch.relu(self.layer1(x))
        x=self.dropout(x)
        x=torch.relu(self.layer2(x))
        x=self.dropout(x)
        x=self.layer3(x)
        return x
	\end{minted}
\end{frame}

\begin{frame}[fragile]{模型结构}
	仍旧是三层全连接层, 但是第二层和第三层之间使用了Dropout率为0.5的dropout层, 随机
	丢弃一半输入数据来帮助避免过拟合.

	\begin{block}{学习率规划器}
		使用`StepLR'规划器来在训练过程中调节学习率. 特别的, 学习率每100轮被乘以一个参数
		`gamma'(此处为0.1). 随时间降低学习率可以帮助细化模型的权重, 使其更加贴近最优方案.
	\end{block}
\end{frame}

\section{MNIST手写数字识别}
\begin{frame}[fragile]{CNN进行MNIST手写数字识别}
	借助PyTorch框架, 定义一个CNN进行MNIST手写数字识别.
\end{frame}

\begin{frame}[fragile]{模型结构}
	\begin{minted}[breaklines,tabsize=4]{python}
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        self.conv_layers = nn.Sequential(
            # (n, 1, 28, 28) -> (n,32,28,28)
            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            # (n,32,28,28)->(n,32,14,14)
            nn.MaxPool2d(kernel_size=2, stride=2),
            # (n,32,14,14)->(n,64,14,14)
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            # (n,64,14,14)->(n,64,7,7)
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
	\end{minted}

\end{frame}
\begin{frame}[fragile]
	\frametitle{模型结构}

	\begin{minted}[breaklines,tabsize=4]{python}
        self.fc_layers = nn.Sequential(
            # (n,64*7*7)->(n,128)
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            # (n,128)->(n,10)
            nn.Linear(128, 10),
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # flatten the tensor
        x = self.fc_layers(x)
        return x
	\end{minted}

\end{frame}

\begin{frame}[fragile]{模型结构}
	\begin{enumerate}
		\item<only@1> \textbf{卷积层(`conv\_layers')}
			这些层用来处理输入图像的空间信息, 检测局部图案, 边缘, 纹理等. 通过级联卷积和池化操作以分层方式转换图像.
			\begin{enumerate}
				\item \textbf{第一卷积层}
				      \begin{itemize}
					      \item \textbf{卷积 } 这一层使用单个通道(因为MNIST图片为灰度图片), 将其转化为32个通道, 借助$3\times 3$卷积核. 大小为1的填充确保了输出和输入的宽和高相同, 因此输出形状为$(n,32,28,28)$.
					      \item \textbf{激活函数 } `ReLU'激活函数引入了非线性.
					      \item \textbf{池化 } 之后的`MaxPool2d'操作使用了步长为2的池化核大小为$2\times 2$
					            的池化操作来降低空间维度大小到原来的一半, 使得输出形状为$(n,32,14,14)$.
				      \end{itemize}
				\item \textbf{第二卷积层}
				      \begin{itemize}
					      \item 卷积: 将前一层的32个通道借助$3\times 3$的卷积核转化为64通道, 由于填充为1, 保留了原本的维度.
					      \item 激活函数: ReLU.
					      \item 池化: 另一个`MaxPool2d'操作将其维度降低到原来的一半, 输出形状为$(n,64,7,7)$.
				      \end{itemize}
			\end{enumerate}
		\item<only@2> \textbf{全连接层(`fc\_layers')}
			\begin{itemize}
				\item 这些层分析卷积层提取和学到的特征, 将其分类到10个可能的数字类中.
			\end{itemize}
			\begin{enumerate}
				\item \textbf{第一个全连接层 }
				      \begin{itemize}
					      \item \textbf{扁平化} 在将其从卷积层传入全连接层之前, 将3D张量$(64,7,7)$扁平化到1D的3136个元素的张量.
					      \item \textbf{线性变形 } 将3136个节点降低到128个节点.
					      \item \textbf{激活函数 } 再次使用`ReLU'函数来引入非线性.
				      \end{itemize}
				\item \textbf{第二个全连接层 }
				      \begin{itemize}
					      \item 使用了128个来自前一层的节点, 将其减少到10个节点, 对应10个MNIST中可能的数字类(0到9).
				      \end{itemize}
			\end{enumerate}
	\end{enumerate}
\end{frame}

\begin{frame}[fragile]{模型训练}
	接下来介绍对模型的训练. 首先将模型和数据移动到CUDA GPU上, 然后对模型进行训练.
	\begin{minted}[breaklines,tabsize=4]{python}
device = torch.device("cuda" if torch.cuda.is_available else "cpu")
# initialize the model
model = CNN().to(device)
	\end{minted}
	首先判断cuda是否可用, 如果可用则使用其进行加速. 否则在cpu上进行训练.
\end{frame}

\begin{frame}[fragile]{损失和优化器配置}
	\begin{minted}[breaklines,tabsize=4]{python}
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
	\end{minted}
	\begin{itemize}
		\item Loss函数使用交叉熵, 对于分类任务这是常用的;
		\item 使用Adam优化器, 以及0.001的学习率来调节模型参数;
	\end{itemize}
\end{frame}
\begin{frame}[fragile]{训练循环}
	\begin{minted}[breaklines,tabsize=4]{python}
num_epochs = 10
for epoch in range(num_epochs):  # loop over the dataset multiple times
    running_loss = 0.0
    model.train()  # set model to train mode
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        # forward, backward, optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    print(f"Epoch {epoch+1},loss: {running_loss/len(trainloader)}")
	\end{minted}
\end{frame}

\begin{frame}[fragile]{训练循环}
	\begin{itemize}
		\item 模型进行10轮训练;
		\item 在数据集上迭代之前, `running\_loss'被设置为0来累加一轮的损失.
		\item 借助`model.train()'将模型设置到训练模式. 该模式影响dropout或者batch normalization层这种在训练时和测试时表现不同的层.
		\item 对来自`trainloader'数据的每个批次(`inputs'和对应的`labels'):
		      \begin{itemize}
			      \item 数据被转移到配置的装置(CUDA或CPU);
			      \item 梯度被清零来确保没有来自前一次迭代的累加;
			      \item 一个前向过程计算模型的预测(`outputs');
			      \item loss被在预测和真实标签之间计算出来;
			      \item 一个反向传播过程计算loss与对应的模型参数的梯度值;
			      \item 优化器基于计算的梯度更新模型的参数;
			      \item 该批次的loss被累加到`running\_loss';
		      \end{itemize}
		\item 在所有的批次被处理后, 该轮次的平均loss被打印.
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{训练循环的验证阶段}
	\begin{minted}[breaklines,tabsize=4]{python}
	...
    model.eval()  # set the model to evaluation mode
    with torch.no_grad():
        running_loss = 0.0
        correct_predictions = 0
        total_predictions = 0
        for inputs, labels in validloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_predictions += labels.size(0)
            correct_predictions += (predicted == labels).sum().item()
        print(
            f"Validation loss: {running_loss/len(validloader)}, Validation accuracy: {correct_predictions/total_predictions*100}%"
        )
	\end{minted}
\end{frame}

\begin{frame}[fragile]{训练循环的验证阶段}
	\begin{itemize}
		\item 模型被借助`model.eval()'设置为验证模式. 这确保了dropout和batch normalization等层工作在验证模式;
		\item `torch.no\_grad()'确保了在这一阶段不会计算梯度, 减少了内存消耗;
		\item 对来自`validloader'的每一批次数据:
		      \begin{itemize}
			      \item 数据被转移到device上;
			      \item 一个前向传播过程计算了模型预测;
			      \item loss被计算并添加到`running\_loss';
			      \item 预测值被与真实标签比较来确定准确度.
		      \end{itemize}
		\item 在所有的批次被处理完之后, 平均的验证损失和准确率都被打印;
	\end{itemize}
\end{frame}
\end{document}
