{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Emotion Recognition using CNN\n",
    "> 面部表情识别， CNN网络， Pytorch搭建， GPU训练\n",
    "![](../docs/figures/surprised.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 09:49:02.159971: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"../dataset/train\"\n",
    "val_dir = \"../dataset/test\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 09:49:07.924673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.949355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.949664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.950856: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 09:49:07.951713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.951954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.952102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.997469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.997661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.997796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 09:49:07.997888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "# Layers in Sequential are stacked on top of each other\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# 2D Conv layer with 32 filters of size 3x3. Input shape is (48,48,1), for\n",
    "# grayscale images of size 48x48.\n",
    "emotion_model.add(\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(48, 48, 1))\n",
    ")\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation=\"relu\"))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Dense(7, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3283901/729160076.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  emotion_model_info = emotion_model.fit_generator(\n",
      "2023-07-17 09:49:11.474218: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-07-17 09:49:11.595667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8800\n",
      "2023-07-17 09:49:12.058971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 9s 16ms/step - loss: 1.7829 - accuracy: 0.2728 - val_loss: 1.6283 - val_accuracy: 0.3758\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.5656 - accuracy: 0.3932 - val_loss: 1.5067 - val_accuracy: 0.4210\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.4544 - accuracy: 0.4454 - val_loss: 1.4125 - val_accuracy: 0.4636\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.3752 - accuracy: 0.4788 - val_loss: 1.3264 - val_accuracy: 0.5008\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.3161 - accuracy: 0.5021 - val_loss: 1.2807 - val_accuracy: 0.5138\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.2694 - accuracy: 0.5229 - val_loss: 1.2504 - val_accuracy: 0.5229\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.2245 - accuracy: 0.5398 - val_loss: 1.2287 - val_accuracy: 0.5306\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.1906 - accuracy: 0.5546 - val_loss: 1.1861 - val_accuracy: 0.5467\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.1604 - accuracy: 0.5652 - val_loss: 1.1785 - val_accuracy: 0.5587\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.1277 - accuracy: 0.5763 - val_loss: 1.1546 - val_accuracy: 0.5621\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.0971 - accuracy: 0.5899 - val_loss: 1.1342 - val_accuracy: 0.5692\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 1.0707 - accuracy: 0.6007 - val_loss: 1.1243 - val_accuracy: 0.5752\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 1.0448 - accuracy: 0.6093 - val_loss: 1.1206 - val_accuracy: 0.5752\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 1.0148 - accuracy: 0.6238 - val_loss: 1.1092 - val_accuracy: 0.5845\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.9858 - accuracy: 0.6344 - val_loss: 1.1015 - val_accuracy: 0.5830\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 7s 14ms/step - loss: 0.9583 - accuracy: 0.6470 - val_loss: 1.0874 - val_accuracy: 0.5932\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 7s 14ms/step - loss: 0.9319 - accuracy: 0.6563 - val_loss: 1.0878 - val_accuracy: 0.5961\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 8s 17ms/step - loss: 0.9062 - accuracy: 0.6670 - val_loss: 1.0768 - val_accuracy: 0.6014\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.8755 - accuracy: 0.6793 - val_loss: 1.0733 - val_accuracy: 0.6060\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.8385 - accuracy: 0.6919 - val_loss: 1.0764 - val_accuracy: 0.6063\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.8162 - accuracy: 0.6991 - val_loss: 1.0786 - val_accuracy: 0.6098\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 0.7840 - accuracy: 0.7159 - val_loss: 1.0843 - val_accuracy: 0.6060\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.7522 - accuracy: 0.7267 - val_loss: 1.0808 - val_accuracy: 0.6087\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.7213 - accuracy: 0.7398 - val_loss: 1.0977 - val_accuracy: 0.6105\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.6940 - accuracy: 0.7495 - val_loss: 1.0835 - val_accuracy: 0.6140\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.6590 - accuracy: 0.7638 - val_loss: 1.0951 - val_accuracy: 0.6151\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.6285 - accuracy: 0.7723 - val_loss: 1.1041 - val_accuracy: 0.6129\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.6040 - accuracy: 0.7843 - val_loss: 1.1255 - val_accuracy: 0.6187\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.5709 - accuracy: 0.7946 - val_loss: 1.1197 - val_accuracy: 0.6225\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 7s 14ms/step - loss: 0.5458 - accuracy: 0.8023 - val_loss: 1.1368 - val_accuracy: 0.6205\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.5127 - accuracy: 0.8163 - val_loss: 1.1554 - val_accuracy: 0.6200\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.4888 - accuracy: 0.8250 - val_loss: 1.1744 - val_accuracy: 0.6164\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.4644 - accuracy: 0.8356 - val_loss: 1.1698 - val_accuracy: 0.6190\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.4423 - accuracy: 0.8418 - val_loss: 1.1938 - val_accuracy: 0.6207\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.4182 - accuracy: 0.8520 - val_loss: 1.2137 - val_accuracy: 0.6210\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.3961 - accuracy: 0.8569 - val_loss: 1.2180 - val_accuracy: 0.6214\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 6s 14ms/step - loss: 0.3731 - accuracy: 0.8687 - val_loss: 1.2565 - val_accuracy: 0.6198\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.3563 - accuracy: 0.8725 - val_loss: 1.2841 - val_accuracy: 0.6250\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.3360 - accuracy: 0.8800 - val_loss: 1.2896 - val_accuracy: 0.6208\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.3217 - accuracy: 0.8863 - val_loss: 1.3039 - val_accuracy: 0.6223\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.3116 - accuracy: 0.8896 - val_loss: 1.3082 - val_accuracy: 0.6164\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2943 - accuracy: 0.8970 - val_loss: 1.3191 - val_accuracy: 0.6256\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2774 - accuracy: 0.9020 - val_loss: 1.3495 - val_accuracy: 0.6219\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2696 - accuracy: 0.9067 - val_loss: 1.3711 - val_accuracy: 0.6239\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2595 - accuracy: 0.9096 - val_loss: 1.4030 - val_accuracy: 0.6184\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2426 - accuracy: 0.9166 - val_loss: 1.3908 - val_accuracy: 0.6204\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 0.2330 - accuracy: 0.9193 - val_loss: 1.4336 - val_accuracy: 0.6210\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 7s 16ms/step - loss: 0.2309 - accuracy: 0.9182 - val_loss: 1.4185 - val_accuracy: 0.6189\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2127 - accuracy: 0.9279 - val_loss: 1.4498 - val_accuracy: 0.6165\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 7s 15ms/step - loss: 0.2093 - accuracy: 0.9273 - val_loss: 1.4822 - val_accuracy: 0.6180\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "emotion_model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=0.0001, decay=1e-6),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178 // 64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model weigh\n",
    "import datetime\n",
    "emotion_model.save_weights(f'model-{datetime.date.today()}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@2201.998] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@2201.998] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n",
      "[ERROR:0@2201.998] global persistence.cpp:512 open Can't open file: 'haarcascade_frontalface_default.xmlhaarcascade_frontalface_default.xml' in read mode\n"
     ]
    }
   ],
   "source": [
    "# using OpenCV haarcascade xml detect the bounding boxes of face in the webcam and predict the emotions\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgusted\",\n",
    "    2: \"Fearful\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprised\",\n",
    "}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# load the cascade classifier before loop\n",
    "haarcascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "bounding_box = cv2.CascadeClassifier(haarcascade_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(\n",
    "        gray_frame, scaleFactor=1.3, minNeighbors=5\n",
    "    )\n",
    "\n",
    "    for x, y, w, h in num_faces:\n",
    "        cv2.rectangle(frame, (x, y - 50), (x + w, y + h + 10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y : y + h, x : x + w]\n",
    "        cropped_img = np.expand_dims(\n",
    "            np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0\n",
    "        )\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            emotion_dict[maxindex],\n",
    "            (x + 20, y - 60),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    cv2.imshow(\n",
    "        \"Video\", cv2.resize(frame, (1200, 860), interpolation=cv2.INTER_CUBIC)\n",
    "    )\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion detection result\n",
    "![](../docs/figures/surprised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for GUI and mapping with emojis\n",
    "save the emojis corresponding to each of the seven enotions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot open the camera(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@43796.046] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@43796.047] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 160\u001b[0m\n\u001b[1;32m    152\u001b[0m root[\u001b[39m\"\u001b[39m\u001b[39mbg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m exitbutton \u001b[39m=\u001b[39m Button(\n\u001b[1;32m    154\u001b[0m     root,\n\u001b[1;32m    155\u001b[0m     text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuit\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     font\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39marial\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m25\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbold\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    159\u001b[0m )\u001b[39m.\u001b[39mpack(side\u001b[39m=\u001b[39mBOTTOM)\n\u001b[0;32m--> 160\u001b[0m show_vid()\n\u001b[1;32m    161\u001b[0m show_vid2()\n\u001b[1;32m    162\u001b[0m root\u001b[39m.\u001b[39mmainloop()\n",
      "Cell \u001b[0;32mIn[23], line 64\u001b[0m, in \u001b[0;36mshow_vid\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCannot open the camera(0)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m flag1, frame1 \u001b[39m=\u001b[39m cap1\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 64\u001b[0m frame1 \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(frame1, (\u001b[39m600\u001b[39;49m, \u001b[39m500\u001b[39;49m))\n\u001b[1;32m     66\u001b[0m haarcascade_path \u001b[39m=\u001b[39m (\n\u001b[1;32m     67\u001b[0m     cv2\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mhaarcascades \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhaarcascade_frontalface_default.xml\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m bounding_box \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(haarcascade_path)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(48, 48, 1))\n",
    ")\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation=\"relu\"))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation=\"relu\"))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation=\"softmax\"))\n",
    "emotion_model.load_weights(\"./model-2023-07-17.h5\")\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {\n",
    "    0: \"   Angry   \",\n",
    "    1: \"Disgusted\",\n",
    "    2: \"  Fearful  \",\n",
    "    3: \"   Happy   \",\n",
    "    4: \"  Neutral  \",\n",
    "    5: \"    Sad    \",\n",
    "    6: \"Surprised\",\n",
    "}\n",
    "emoji_dist = {\n",
    "    0: \"./emojis/angry.png\",\n",
    "    2: \"./emojis/disgusted.png\",\n",
    "    2: \"./emojis/fearful.png\",\n",
    "    3: \"./emojis/happy.png\",\n",
    "    4: \"./emojis/neutral.png\",\n",
    "    5: \"./emojis/sad.png\",\n",
    "    6: \"./emojis/surpriced.png\",\n",
    "}\n",
    "global last_frame1\n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text = [0]\n",
    "\n",
    "\n",
    "def show_vid():\n",
    "    cap1 = cv2.VideoCapture(0)\n",
    "    if not cap1.isOpened():\n",
    "        print(\"Cannot open the camera(0)\")\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1, (600, 500))\n",
    "\n",
    "    haarcascade_path = (\n",
    "        cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "    )\n",
    "    bounding_box = cv2.CascadeClassifier(haarcascade_path)\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(\n",
    "        gray_frame, scaleFactor=1.3, minNeighbors=5\n",
    "    )\n",
    "    for x, y, w, h in num_faces:\n",
    "        cv2.rectangle(frame1, (x, y - 50), (x + w, y + h + 10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y : y + h, x : x + w]\n",
    "        cropped_img = np.expand_dims(\n",
    "            np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0\n",
    "        )\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(\n",
    "            frame1,\n",
    "            emotion_dict[maxindex],\n",
    "            (x + 20, y - 60),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        show_text[0] = maxindex\n",
    "    if flag1 is None:\n",
    "        print(\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, show_vid)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        exit()\n",
    "\n",
    "\n",
    "def show_vid2():\n",
    "    frame2 = cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "    img2 = Image.fromarray(frame2)\n",
    "    imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2 = imgtk2\n",
    "    lmain3.configure(\n",
    "        text=emotion_dict[show_text[0]], font=(\"arial\", 45, \"bold\")\n",
    "    )\n",
    "\n",
    "    lmain2.configure(image=imgtk2)\n",
    "    lmain2.after(10, show_vid2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    # img = ImageTk.PhotoImage(Image.open(\"../dataset/test/happy/PrivateTest_10077120.jpg\"))\n",
    "    # heading = Label(root, image=img, bg=\"black\")\n",
    "    # heading = Label(root, image=img, bg=\"black\")\n",
    "\n",
    "    # heading.pack()\n",
    "    heading2 = Label(\n",
    "        root,\n",
    "        text=\"Photo to Emoji\",\n",
    "        pady=20,\n",
    "        font=(\"arial\", 45, \"bold\"),\n",
    "        bg=\"black\",\n",
    "        fg=\"#CDCDCD\",\n",
    "    )\n",
    "\n",
    "    heading2.pack()\n",
    "    lmain = tk.Label(master=root, padx=50, bd=10)\n",
    "    lmain2 = tk.Label(master=root, bd=10)\n",
    "    lmain3 = tk.Label(master=root, bd=10, fg=\"#CDCDCD\", bg=\"black\")\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50, y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960, y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900, y=350)\n",
    "\n",
    "    root.title(\"Photo To Emoji\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root[\"bg\"] = \"black\"\n",
    "    exitbutton = Button(\n",
    "        root,\n",
    "        text=\"Quit\",\n",
    "        fg=\"red\",\n",
    "        command=root.destroy,\n",
    "        font=(\"arial\", 25, \"bold\"),\n",
    "    ).pack(side=BOTTOM)\n",
    "    show_vid()\n",
    "    show_vid2()\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
