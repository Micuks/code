在本实验中, 选取北京, 上海, 广州, 深圳和潍坊五个城市, 借助Scrapy框架设计爬虫, 部
署在Scrapyd上运行爬取; 爬取数据以定义的item结构保存, 通过pipeline传输到使用
SQLite3建立的数据库进行去重, 断点续爬和持久化. 使用pandas与matplotlib进行数据分
析和可视化.

由于链家100页只有3000个房源, 远小于房源总数, 直接对租房房源进行爬取将遗漏大量房
源信息. 因此为了尽可能多的爬取房源, 制作了三个爬虫. 分别爬取板块(商圈, business
area), 小区(community)和租房房源(rental).在爬取板块的同时, 爬虫可以\textbf{自动
收集行政区划分}, 小区指北京邮电大学家属小区这一级, 租房房源指具体的出租房源信息.
由于一个小区内不会有超过3000各租房房源,因此通过这一划分方式, 可以尽可能的避免这
一限制造成的爬取不充分.

由于实现了上述的较高程度的自动化, 爬虫对城市的增加有极强的扩展性. 如果需要添加新
的城市, 只需要添加新城市的租房url即可, 而不需要手动添加待添加城市的具体行政区划
等信息.

爬虫共计爬取商圈800个, 其中北京255个, 上海172个, 广州227个, 深圳74个, 潍坊72个.
小区41262个, 其中上海17211个, 北京10213个, 广州6575个, 深圳4098个, 潍坊3165个.租
房房源共175570个, 其中北京31095个, 上海22998个, 广州74387个, 深圳37840个, 潍坊
9250个. 由于商圈, 小区和房源的url中都有其唯一编码, 故在插入SQLite的时候借助
unique关键字对其进行去重.