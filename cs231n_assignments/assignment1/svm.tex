\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    \usepackage{xeCJK}

    \setCJKmainfont{Adobe Song Std}
    \setCJKsansfont{Adobe Song Std}
    \setCJKmonofont{Adobe Song Std}
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{svm}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} This mounts your Google Drive to the Colab VM.}
\PY{k+kn}{from} \PY{n+nn}{google}\PY{n+nn}{.}\PY{n+nn}{colab} \PY{k+kn}{import} \PY{n}{drive}
\PY{n}{drive}\PY{o}{.}\PY{n}{mount}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/content/drive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} TODO: Enter the foldername in your Drive where you have saved the unzipped}
\PY{c+c1}{\PYZsh{} assignment folder, e.g. \PYZsq{}cs231n/assignments/assignment1/\PYZsq{}}
\PY{n}{FOLDERNAME} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cs231n/assignments/assignment1/}\PY{l+s+s1}{\PYZsq{}}
\PY{k}{assert} \PY{n}{FOLDERNAME} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[!] Enter the foldername.}\PY{l+s+s2}{\PYZdq{}}

\PY{c+c1}{\PYZsh{} Now that we\PYZsq{}ve mounted your Drive, this ensures that}
\PY{c+c1}{\PYZsh{} the Python interpreter of the Colab VM can load}
\PY{c+c1}{\PYZsh{} python files from within it.}
\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/content/drive/My Drive/}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{FOLDERNAME}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} This downloads the CIFAR\PYZhy{}10 dataset to your Drive}
\PY{c+c1}{\PYZsh{} if it doesn\PYZsq{}t already exist.}
\PY{o}{\PYZpc{}}\PY{n}{cd} \PY{o}{/}\PY{n}{content}\PY{o}{/}\PY{n}{drive}\PY{o}{/}\PY{n}{My}\PYZbs{} \PY{n}{Drive}\PY{o}{/}\PY{err}{\PYZdl{}}\PY{n}{FOLDERNAME}\PY{o}{/}\PY{n}{cs231n}\PY{o}{/}\PY{n}{datasets}\PY{o}{/}
\PY{err}{!}\PY{n}{bash} \PY{n}{get\PYZus{}datasets}\PY{o}{.}\PY{n}{sh}
\PY{o}{\PYZpc{}}\PY{n}{cd} \PY{o}{/}\PY{n}{content}\PY{o}{/}\PY{n}{drive}\PY{o}{/}\PY{n}{My}\PYZbs{} \PY{n}{Drive}\PY{o}{/}\PY{err}{\PYZdl{}}\PY{n}{FOLDERNAME}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Drive already mounted at /content/drive; to attempt to forcibly remount, call
drive.mount("/content/drive", force\_remount=True).
/content/drive/My Drive/cs231n/assignments/assignment1/cs231n/datasets
/content/drive/My Drive/cs231n/assignments/assignment1
    \end{Verbatim}

    \hypertarget{multiclass-support-vector-machine-exercise}{%
\section{Multiclass Support Vector Machine
exercise}\label{multiclass-support-vector-machine-exercise}}

\emph{Complete and hand in this completed worksheet (including its
outputs and any supporting code outside of the worksheet) with your
assignment submission. For more details see the
\href{http://vision.stanford.edu/teaching/cs231n/assignments.html}{assignments
page} on the course website.}

In this exercise you will:

\begin{itemize}
\tightlist
\item
  implement a fully-vectorized \textbf{loss function} for the SVM
\item
  implement the fully-vectorized expression for its \textbf{analytic
  gradient}
\item
  \textbf{check your implementation} using numerical gradient
\item
  use a validation set to \textbf{tune the learning rate and
  regularization} strength
\item
  \textbf{optimize} the loss function with \textbf{SGD}
\item
  \textbf{visualize} the final learned weights
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Run some setup code for this notebook.}
\PY{k+kn}{import} \PY{n+nn}{random}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{cs231n}\PY{n+nn}{.}\PY{n+nn}{data\PYZus{}utils} \PY{k+kn}{import} \PY{n}{load\PYZus{}CIFAR10}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{c+c1}{\PYZsh{} This is a bit of magic to make matplotlib figures appear inline in the}
\PY{c+c1}{\PYZsh{} notebook rather than in a new window.}
\PY{o}{\PYZpc{}}\PY{n}{matplotlib} \PY{n}{inline}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{10.0}\PY{p}{,} \PY{l+m+mf}{8.0}\PY{p}{)} \PY{c+c1}{\PYZsh{} set default size of plots}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image.interpolation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}
\PY{n}{plt}\PY{o}{.}\PY{n}{rcParams}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{image.cmap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} Some more magic so that the notebook will reload external python modules;}
\PY{c+c1}{\PYZsh{} see http://stackoverflow.com/questions/1907993/autoreload\PYZhy{}of\PYZhy{}modules\PYZhy{}in\PYZhy{}ipython}
\PY{o}{\PYZpc{}}\PY{n}{load\PYZus{}ext} \PY{n}{autoreload}
\PY{o}{\PYZpc{}}\PY{n}{autoreload} \PY{l+m+mi}{2}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The autoreload extension is already loaded. To reload it, use:
  \%reload\_ext autoreload
    \end{Verbatim}

    \hypertarget{cifar-10-data-loading-and-preprocessing}{%
\subsection{CIFAR-10 Data Loading and
Preprocessing}\label{cifar-10-data-loading-and-preprocessing}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Load the raw CIFAR\PYZhy{}10 data.}
\PY{n}{cifar10\PYZus{}dir} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cs231n/datasets/cifar\PYZhy{}10\PYZhy{}batches\PYZhy{}py}\PY{l+s+s1}{\PYZsq{}}

\PY{c+c1}{\PYZsh{} Cleaning up variables to prevent loading data multiple times (which may cause memory issue)}
\PY{k}{try}\PY{p}{:}
   \PY{k}{del} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}
   \PY{k}{del} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}
   \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Clear previously loaded data.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{except}\PY{p}{:}
   \PY{k}{pass}

\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{load\PYZus{}CIFAR10}\PY{p}{(}\PY{n}{cifar10\PYZus{}dir}\PY{p}{)}

\PY{c+c1}{\PYZsh{} As a sanity check, we print out the size of the training and test data.}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Clear previously loaded data.
Training data shape:  (50000, 32, 32, 3)
Training labels shape:  (50000,)
Test data shape:  (10000, 32, 32, 3)
Test labels shape:  (10000,)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Visualize some examples from the dataset.}
\PY{c+c1}{\PYZsh{} We show a few examples of training images from each class.}
\PY{n}{classes} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{plane}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{car}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bird}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{deer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{frog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{horse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ship}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{truck}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{classes}\PY{p}{)}
\PY{n}{samples\PYZus{}per\PYZus{}class} \PY{o}{=} \PY{l+m+mi}{7}
\PY{k}{for} \PY{n}{y}\PY{p}{,} \PY{n+nb+bp}{cls} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{classes}\PY{p}{)}\PY{p}{:}
    \PY{n}{idxs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{flatnonzero}\PY{p}{(}\PY{n}{y\PYZus{}train} \PY{o}{==} \PY{n}{y}\PY{p}{)}
    \PY{n}{idxs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{idxs}\PY{p}{,} \PY{n}{samples\PYZus{}per\PYZus{}class}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{idxs}\PY{p}{)}\PY{p}{:}
        \PY{n}{plt\PYZus{}idx} \PY{o}{=} \PY{n}{i} \PY{o}{*} \PY{n}{num\PYZus{}classes} \PY{o}{+} \PY{n}{y} \PY{o}{+} \PY{l+m+mi}{1}
        \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n}{samples\PYZus{}per\PYZus{}class}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{,} \PY{n}{plt\PYZus{}idx}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{idx}\PY{p}{]}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{if} \PY{n}{i} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n+nb+bp}{cls}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{svm_files/svm_5_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Split the data into train, val, and test sets. In addition we will}
\PY{c+c1}{\PYZsh{} create a small development set as a subset of the training data;}
\PY{c+c1}{\PYZsh{} we can use this for development so our code runs faster.}
\PY{n}{num\PYZus{}training} \PY{o}{=} \PY{l+m+mi}{49000}
\PY{n}{num\PYZus{}validation} \PY{o}{=} \PY{l+m+mi}{1000}
\PY{n}{num\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{1000}
\PY{n}{num\PYZus{}dev} \PY{o}{=} \PY{l+m+mi}{500}

\PY{c+c1}{\PYZsh{} Our validation set will be num\PYZus{}validation points from the original}
\PY{c+c1}{\PYZsh{} training set.}
\PY{n}{mask} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{p}{,} \PY{n}{num\PYZus{}training} \PY{o}{+} \PY{n}{num\PYZus{}validation}\PY{p}{)}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
\PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Our training set will be the first num\PYZus{}train points from the original}
\PY{c+c1}{\PYZsh{} training set.}
\PY{n}{mask} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{p}{)}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}

\PY{c+c1}{\PYZsh{} We will also make a development set, which is a small subset of}
\PY{c+c1}{\PYZsh{} the training set.}
\PY{n}{mask} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{num\PYZus{}training}\PY{p}{,} \PY{n}{num\PYZus{}dev}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
\PY{n}{y\PYZus{}dev} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{mask}\PY{p}{]}

\PY{c+c1}{\PYZsh{} We use the first num\PYZus{}test points of the original test set as our}
\PY{c+c1}{\PYZsh{} test set.}
\PY{n}{mask} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}test}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{p}{[}\PY{n}{mask}\PY{p}{]}
\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{p}{[}\PY{n}{mask}\PY{p}{]}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test labels shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train data shape:  (49000, 32, 32, 3)
Train labels shape:  (49000,)
Validation data shape:  (1000, 32, 32, 3)
Validation labels shape:  (1000,)
Test data shape:  (1000, 32, 32, 3)
Test labels shape:  (1000,)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Preprocessing: reshape the image data into rows}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
\PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} As a sanity check, print out the shapes of the data}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dev data shape: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Training data shape:  (49000, 3072)
Validation data shape:  (1000, 3072)
Test data shape:  (1000, 3072)
dev data shape:  (500, 3072)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Preprocessing: subtract the mean image}
\PY{c+c1}{\PYZsh{} first: compute the image mean based on the training data}
\PY{n}{mean\PYZus{}image} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{mean\PYZus{}image}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} print a few of the elements}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{mean\PYZus{}image}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} visualize the mean image}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} second: subtract the mean image from train and test data}
\PY{n}{X\PYZus{}train} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
\PY{n}{X\PYZus{}val} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
\PY{n}{X\PYZus{}test} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}
\PY{n}{X\PYZus{}dev} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{mean\PYZus{}image}

\PY{c+c1}{\PYZsh{} third: append the bias dimension of ones (i.e. bias trick) so that our SVM}
\PY{c+c1}{\PYZsh{} only has to worry about optimizing a single weight matrix W.}
\PY{c+c1}{\PYZsh{} 新增一列全1,以消去参数b}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{X\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{X\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[130.64189796 135.98173469 132.47391837 130.05569388 135.34804082
 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347]
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{svm_files/svm_8_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
(49000, 3073) (1000, 3073) (1000, 3073) (500, 3073)
    \end{Verbatim}

    \hypertarget{svm-classifier}{%
\subsection{SVM Classifier}\label{svm-classifier}}

Your code for this section will all be written inside
\texttt{cs231n/classifiers/linear\_svm.py}.

As you can see, we have prefilled the function \texttt{svm\_loss\_naive}
which uses for loops to evaluate the multiclass SVM loss function.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Evaluate the naive implementation of the loss we provided for you:}
\PY{k+kn}{from} \PY{n+nn}{cs231n}\PY{n+nn}{.}\PY{n+nn}{classifiers}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}svm} \PY{k+kn}{import} \PY{n}{svm\PYZus{}loss\PYZus{}naive}
\PY{k+kn}{import} \PY{n+nn}{time}

\PY{c+c1}{\PYZsh{} generate a random SVM weight matrix of small numbers}
\PY{n}{W} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{3073}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)} \PY{o}{*} \PY{l+m+mf}{0.0001} 

\PY{n}{loss}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.000005}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{loss: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{loss}\PY{p}{,} \PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
loss: 9.401189
    \end{Verbatim}

    The \texttt{grad} returned from the function above is right now all
zero. Derive and implement the gradient for the SVM cost function and
implement it inline inside the function \texttt{svm\_loss\_naive}. You
will find it helpful to interleave your new code inside the existing
function.

To check that you have correctly implemented the gradient, you can
numerically estimate the gradient of the loss function and compare the
numeric estimate to the gradient that you computed. We have provided
code that does this for you:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Once you\PYZsq{}ve implemented the gradient, recompute it with the code below}
\PY{c+c1}{\PYZsh{} and gradient check it with the function we provided for you}

\PY{c+c1}{\PYZsh{} Compute the loss and its gradient at W.}
\PY{n}{loss}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Numerically compute the gradient along several randomly chosen dimensions, and}
\PY{c+c1}{\PYZsh{} compare them with your analytically computed gradient. The numbers should match}
\PY{c+c1}{\PYZsh{} almost exactly along all dimensions.}
\PY{k+kn}{from} \PY{n+nn}{cs231n}\PY{n+nn}{.}\PY{n+nn}{gradient\PYZus{}check} \PY{k+kn}{import} \PY{n}{grad\PYZus{}check\PYZus{}sparse}
\PY{n}{f} \PY{o}{=} \PY{k}{lambda} \PY{n}{w}\PY{p}{:} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{grad\PYZus{}numerical} \PY{o}{=} \PY{n}{grad\PYZus{}check\PYZus{}sparse}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{grad}\PY{p}{)}

\PY{c+c1}{\PYZsh{} do the gradient check once again with regularization turned on}
\PY{c+c1}{\PYZsh{} you didn\PYZsq{}t forget the regularization gradient did you?}
\PY{n}{loss}\PY{p}{,} \PY{n}{grad} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{5e1}\PY{p}{)}
\PY{n}{f} \PY{o}{=} \PY{k}{lambda} \PY{n}{w}\PY{p}{:} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{5e1}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{grad\PYZus{}numerical} \PY{o}{=} \PY{n}{grad\PYZus{}check\PYZus{}sparse}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{grad}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
numerical: -10.408229 analytic: -10.408229, relative error: 4.402479e-11
numerical: 27.476993 analytic: 27.476993, relative error: 1.855623e-11
numerical: 2.280600 analytic: 2.280600, relative error: 1.469642e-10
numerical: -19.094903 analytic: -19.094903, relative error: 5.184346e-12
numerical: 15.157032 analytic: 15.157032, relative error: 1.666856e-11
numerical: -28.282057 analytic: -28.282057, relative error: 1.052069e-11
numerical: -7.334477 analytic: -7.334477, relative error: 6.552705e-11
numerical: -10.706777 analytic: -10.706777, relative error: 3.246013e-11
numerical: -3.125464 analytic: -3.125464, relative error: 1.633782e-10
numerical: -1.752699 analytic: -1.752699, relative error: 1.570124e-10
numerical: -12.406846 analytic: -12.406846, relative error: 3.446879e-12
numerical: -28.903089 analytic: -28.903089, relative error: 7.962022e-13
numerical: 1.964044 analytic: 1.964044, relative error: 1.052466e-10
numerical: -53.543882 analytic: -53.543882, relative error: 5.434979e-12
numerical: 17.290358 analytic: 17.290358, relative error: 7.998783e-12
numerical: 3.992767 analytic: 3.992767, relative error: 3.401323e-11
numerical: 39.058781 analytic: 39.058781, relative error: 1.458996e-11
numerical: 30.773074 analytic: 30.773074, relative error: 4.611024e-12
numerical: 7.486540 analytic: 7.486540, relative error: 1.810384e-11
numerical: 1.236683 analytic: 1.236683, relative error: 3.594599e-10
    \end{Verbatim}

    \textbf{Inline Question 1}

It is possible that once in a while a dimension in the gradcheck will
not match exactly. What could such a discrepancy be caused by? Is it a
reason for concern? What is a simple example in one dimension where a
gradient check could fail? How would change the margin affect of the
frequency of this happening? \emph{Hint: the SVM loss function is not
strictly speaking differentiable}

\(\color{blue}{\textit Your Answer:}\)

SVM损失函数为\(\max(S_j-S_{y_i}+1,0)\),在\(S_j-S_{y_i}+1\)处aSVM损失函数为\(\max(S_j-S_{y_i}+1,0)\),在\(S_j-S_{y_i}+1\)处不可微,所以\(S_j-S_{y_i}\)在0的两侧,\(Loss\)函数值会发生跳变,导致分析方法和数值方法结果不吻合。

如果margin变小,这种情况出现的频率会降低。

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Next implement the function svm\PYZus{}loss\PYZus{}vectorized; for now only compute the loss;}
\PY{c+c1}{\PYZsh{} we will implement the gradient in a moment.}
\PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{loss\PYZus{}naive}\PY{p}{,} \PY{n}{grad\PYZus{}naive} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.000005}\PY{p}{)}
\PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Naive loss: }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{ computed in }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{loss\PYZus{}naive}\PY{p}{,} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}

\PY{k+kn}{from} \PY{n+nn}{cs231n}\PY{n+nn}{.}\PY{n+nn}{classifiers}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}svm} \PY{k+kn}{import} \PY{n}{svm\PYZus{}loss\PYZus{}vectorized}
\PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{loss\PYZus{}vectorized}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}vectorized}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.000005}\PY{p}{)}
\PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Vectorized loss: }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{ computed in }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{loss\PYZus{}vectorized}\PY{p}{,} \PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} The losses should match but your vectorized implementation should be much faster.}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{difference: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{loss\PYZus{}naive} \PY{o}{\PYZhy{}} \PY{n}{loss\PYZus{}vectorized}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Naive loss: 9.401189e+00 computed in 0.128910s
Vectorized loss: 9.401189e+00 computed in 0.013510s
difference: -0.000000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Complete the implementation of svm\PYZus{}loss\PYZus{}vectorized, and compute the gradient}
\PY{c+c1}{\PYZsh{} of the loss function in a vectorized way.}

\PY{c+c1}{\PYZsh{} The naive implementation and the vectorized implementation should match, but}
\PY{c+c1}{\PYZsh{} the vectorized version should still be much faster.}
\PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{grad\PYZus{}naive} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}naive}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.000005}\PY{p}{)}
\PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Naive loss and gradient: computed in }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}

\PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{grad\PYZus{}vectorized} \PY{o}{=} \PY{n}{svm\PYZus{}loss\PYZus{}vectorized}\PY{p}{(}\PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{y\PYZus{}dev}\PY{p}{,} \PY{l+m+mf}{0.000005}\PY{p}{)}
\PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Vectorized loss and gradient: computed in }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} The loss is a single number, so it is easy to compare the values computed}
\PY{c+c1}{\PYZsh{} by the two implementations. The gradient on the other hand is a matrix, so}
\PY{c+c1}{\PYZsh{} we use the Frobenius norm to compare them.}
\PY{c+c1}{\PYZsh{} print(f\PYZsq{}grad\PYZus{}naive:\PYZbs{}n\PYZob{}grad\PYZus{}naive\PYZcb{}\PYZsq{})}
\PY{c+c1}{\PYZsh{} print(f\PYZsq{}grad\PYZus{}vectorized:\PYZbs{}n\PYZob{}grad\PYZus{}vectorized\PYZcb{}\PYZsq{})}
\PY{n}{difference} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad\PYZus{}naive} \PY{o}{\PYZhy{}} \PY{n}{grad\PYZus{}vectorized}\PY{p}{,} \PY{n+nb}{ord}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{difference: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{difference}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Naive loss and gradient: computed in 0.121577s
Vectorized loss and gradient: computed in 0.018211s
difference: 0.000000
    \end{Verbatim}

    \hypertarget{stochastic-gradient-descent}{%
\subsubsection{Stochastic Gradient
Descent}\label{stochastic-gradient-descent}}

We now have vectorized and efficient expressions for the loss, the
gradient and our gradient matches the numerical gradient. We are
therefore ready to do SGD to minimize the loss. Your code for this part
will be written inside
\texttt{cs231n/classifiers/linear\_classifier.py}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} In the file linear\PYZus{}classifier.py, implement SGD in the function}
\PY{c+c1}{\PYZsh{} LinearClassifier.train() and then run it with the code below.}
\PY{k+kn}{from} \PY{n+nn}{cs231n}\PY{n+nn}{.}\PY{n+nn}{classifiers} \PY{k+kn}{import} \PY{n}{LinearSVM}
\PY{n}{svm} \PY{o}{=} \PY{n}{LinearSVM}\PY{p}{(}\PY{p}{)}
\PY{n}{tic} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n}{loss\PYZus{}hist} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{,} \PY{n}{reg}\PY{o}{=}\PY{l+m+mf}{2.5e4}\PY{p}{,}
                      \PY{n}{num\PYZus{}iters}\PY{o}{=}\PY{l+m+mi}{1500}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{toc} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{That took }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{toc} \PY{o}{\PYZhy{}} \PY{n}{tic}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
iteration 0 / 1500: loss 789.920601
iteration 100 / 1500: loss 286.499248
iteration 200 / 1500: loss 107.763667
iteration 300 / 1500: loss 42.455947
iteration 400 / 1500: loss 19.419767
iteration 500 / 1500: loss 10.389614
iteration 600 / 1500: loss 7.432982
iteration 700 / 1500: loss 6.049148
iteration 800 / 1500: loss 5.373677
iteration 900 / 1500: loss 5.360467
iteration 1000 / 1500: loss 5.176213
iteration 1100 / 1500: loss 5.113732
iteration 1200 / 1500: loss 5.311092
iteration 1300 / 1500: loss 5.389933
iteration 1400 / 1500: loss 4.903446
That took 12.533612s
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} A useful debugging strategy is to plot the loss as a function of}
\PY{c+c1}{\PYZsh{} iteration number:}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{loss\PYZus{}hist}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iteration number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{svm_files/svm_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Write the LinearSVM.predict function and evaluate the performance on both the}
\PY{c+c1}{\PYZsh{} training and validation set}
\PY{n}{y\PYZus{}train\PYZus{}pred} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{training accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}train} \PY{o}{==} \PY{n}{y\PYZus{}train\PYZus{}pred}\PY{p}{)}\PY{p}{,} \PY{p}{)}\PY{p}{)}
\PY{n}{y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}val} \PY{o}{==} \PY{n}{y\PYZus{}val\PYZus{}pred}\PY{p}{)}\PY{p}{,} \PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
training accuracy: 0.364959
validation accuracy: 0.387000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Use the validation set to tune hyperparameters (regularization strength and}
\PY{c+c1}{\PYZsh{} learning rate). You should experiment with different ranges for the learning}
\PY{c+c1}{\PYZsh{} rates and regularization strengths; if you are careful you should be able to}
\PY{c+c1}{\PYZsh{} get a classification accuracy of about 0.39 (\PYZgt{} 0.385) on the validation set.}

\PY{c+c1}{\PYZsh{} Note: you may see runtime/overflow warnings during hyper\PYZhy{}parameter search. }
\PY{c+c1}{\PYZsh{} This may be caused by extreme values, and is not a bug.}

\PY{c+c1}{\PYZsh{} results is dictionary mapping tuples of the form}
\PY{c+c1}{\PYZsh{} (learning\PYZus{}rate, regularization\PYZus{}strength) to tuples of the form}
\PY{c+c1}{\PYZsh{} (training\PYZus{}accuracy, validation\PYZus{}accuracy). The accuracy is simply the fraction}
\PY{c+c1}{\PYZsh{} of data points that are correctly classified.}
\PY{n}{results} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
\PY{n}{best\PYZus{}val} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}   \PY{c+c1}{\PYZsh{} The highest validation accuracy that we have seen so far.}
\PY{n}{best\PYZus{}svm} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} The LinearSVM object that achieved the highest validation rate.}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} TODO:                                                                        \PYZsh{}}
\PY{c+c1}{\PYZsh{} Write code that chooses the best hyperparameters by tuning on the validation \PYZsh{}}
\PY{c+c1}{\PYZsh{} set. For each combination of hyperparameters, train a linear SVM on the      \PYZsh{}}
\PY{c+c1}{\PYZsh{} training set, compute its accuracy on the training and validation sets, and  \PYZsh{}}
\PY{c+c1}{\PYZsh{} store these numbers in the results dictionary. In addition, store the best   \PYZsh{}}
\PY{c+c1}{\PYZsh{} validation accuracy in best\PYZus{}val and the LinearSVM object that achieves this  \PYZsh{}}
\PY{c+c1}{\PYZsh{} accuracy in best\PYZus{}svm.                                                        \PYZsh{}}
\PY{c+c1}{\PYZsh{}                                                                              \PYZsh{}}
\PY{c+c1}{\PYZsh{} Hint: You should use a small value for num\PYZus{}iters as you develop your         \PYZsh{}}
\PY{c+c1}{\PYZsh{} validation code so that the SVMs don\PYZsq{}t take much time to train; once you are \PYZsh{}}
\PY{c+c1}{\PYZsh{} confident that your validation code works, you should rerun the validation   \PYZsh{}}
\PY{c+c1}{\PYZsh{} code with a larger value for num\PYZus{}iters.                                      \PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}

\PY{c+c1}{\PYZsh{} Provided as a reference. You may or may not want to change these hyperparameters}
\PY{c+c1}{\PYZsh{} learning\PYZus{}rates = [1e\PYZhy{}7, 5e\PYZhy{}8]}
\PY{n}{learning\PYZus{}rates} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{,}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{,}\PY{l+m+mf}{1e\PYZhy{}7}\PY{p}{)}
\PY{c+c1}{\PYZsh{} regularization\PYZus{}strengths = [2.5e4, 5e4]}
\PY{n}{regularization\PYZus{}strengths} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mf}{1e3}\PY{p}{,}\PY{l+m+mf}{100e3}\PY{p}{,}\PY{l+m+mf}{10e3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

\PY{k}{for} \PY{n}{lr} \PY{o+ow}{in} \PY{n}{learning\PYZus{}rates}\PY{p}{:}
  \PY{k}{for} \PY{n}{reg} \PY{o+ow}{in} \PY{n}{regularization\PYZus{}strengths}\PY{p}{:}
    \PY{n}{new\PYZus{}svm} \PY{o}{=} \PY{n}{LinearSVM}\PY{p}{(}\PY{p}{)}
    \PY{n}{new\PYZus{}svm}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{reg}\PY{o}{=}\PY{n}{reg}\PY{p}{,}
                      \PY{n}{num\PYZus{}iters}\PY{o}{=}\PY{l+m+mi}{1500}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

    \PY{n}{new\PYZus{}y\PYZus{}train\PYZus{}pred} \PY{o}{=} \PY{n}{new\PYZus{}svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
    \PY{n}{new\PYZus{}y\PYZus{}val\PYZus{}pred} \PY{o}{=} \PY{n}{new\PYZus{}svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
  
    \PY{n}{new\PYZus{}train\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}train} \PY{o}{==} \PY{n}{new\PYZus{}y\PYZus{}train\PYZus{}pred}\PY{p}{)}
    \PY{n}{new\PYZus{}val\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}val} \PY{o}{==} \PY{n}{new\PYZus{}y\PYZus{}val\PYZus{}pred}\PY{p}{)}

    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lr }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{ reg }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{ train accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{ val accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}
                \PY{n}{lr}\PY{p}{,} \PY{n}{reg}\PY{p}{,} \PY{n}{new\PYZus{}train\PYZus{}accuracy}\PY{p}{,} \PY{n}{new\PYZus{}val\PYZus{}accuracy}\PY{p}{)}\PY{p}{,}\PY{n}{end}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{n}{results}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{p}{\PYZob{}}\PY{p}{(}\PY{n}{lr}\PY{p}{,} \PY{n}{reg}\PY{p}{)}\PY{p}{:}\PY{p}{(}\PY{n}{new\PYZus{}train\PYZus{}accuracy}\PY{p}{,} \PY{n}{new\PYZus{}val\PYZus{}accuracy}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
    \PY{k}{if} \PY{n}{new\PYZus{}val\PYZus{}accuracy} \PY{o}{\PYZgt{}} \PY{n}{best\PYZus{}val}\PY{p}{:}
      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{new best val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
      \PY{n}{best\PYZus{}val} \PY{o}{=} \PY{n}{new\PYZus{}val\PYZus{}accuracy}
      \PY{n}{best\PYZus{}svm} \PY{o}{=} \PY{n}{new\PYZus{}svm}
  
\PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
    
\PY{c+c1}{\PYZsh{} Print out results.}
\PY{k}{for} \PY{n}{lr}\PY{p}{,} \PY{n}{reg} \PY{o+ow}{in} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{results}\PY{p}{)}\PY{p}{:}
    \PY{n}{train\PYZus{}accuracy}\PY{p}{,} \PY{n}{val\PYZus{}accuracy} \PY{o}{=} \PY{n}{results}\PY{p}{[}\PY{p}{(}\PY{n}{lr}\PY{p}{,} \PY{n}{reg}\PY{p}{)}\PY{p}{]}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lr }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{ reg }\PY{l+s+si}{\PYZpc{}e}\PY{l+s+s1}{ train accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{ val accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}
                \PY{n}{lr}\PY{p}{,} \PY{n}{reg}\PY{p}{,} \PY{n}{train\PYZus{}accuracy}\PY{p}{,} \PY{n}{val\PYZus{}accuracy}\PY{p}{)}\PY{p}{)}
    
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{best validation accuracy achieved during cross\PYZhy{}validation: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{best\PYZus{}val}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.230041 val accuracy: 0.234000

new best val
lr 1.000000e-08 reg 1.100000e+04 train accuracy: 0.241531 val accuracy: 0.268000

new best val
lr 1.000000e-08 reg 2.100000e+04 train accuracy: 0.252918 val accuracy: 0.249000

lr 1.000000e-08 reg 3.100000e+04 train accuracy: 0.268347 val accuracy: 0.266000

lr 1.000000e-08 reg 4.100000e+04 train accuracy: 0.284367 val accuracy: 0.296000

new best val
lr 1.000000e-08 reg 5.100000e+04 train accuracy: 0.299857 val accuracy: 0.308000

new best val
lr 1.000000e-08 reg 6.100000e+04 train accuracy: 0.321286 val accuracy: 0.324000

new best val
lr 1.000000e-08 reg 7.100000e+04 train accuracy: 0.329408 val accuracy: 0.324000

lr 1.000000e-08 reg 8.100000e+04 train accuracy: 0.335612 val accuracy: 0.360000

new best val
lr 1.000000e-08 reg 9.100000e+04 train accuracy: 0.340816 val accuracy: 0.357000

lr 1.100000e-07 reg 1.000000e+03 train accuracy: 0.323408 val accuracy: 0.329000

lr 1.100000e-07 reg 1.100000e+04 train accuracy: 0.383551 val accuracy: 0.396000

new best val
lr 1.100000e-07 reg 2.100000e+04 train accuracy: 0.369857 val accuracy: 0.375000

lr 1.100000e-07 reg 3.100000e+04 train accuracy: 0.356306 val accuracy: 0.358000

lr 1.100000e-07 reg 4.100000e+04 train accuracy: 0.360061 val accuracy: 0.359000

lr 1.100000e-07 reg 5.100000e+04 train accuracy: 0.347918 val accuracy: 0.353000

lr 1.100000e-07 reg 6.100000e+04 train accuracy: 0.353694 val accuracy: 0.357000

lr 1.100000e-07 reg 7.100000e+04 train accuracy: 0.344041 val accuracy: 0.359000

lr 1.100000e-07 reg 8.100000e+04 train accuracy: 0.346776 val accuracy: 0.358000

lr 1.100000e-07 reg 9.100000e+04 train accuracy: 0.344837 val accuracy: 0.354000

lr 2.100000e-07 reg 1.000000e+03 train accuracy: 0.357327 val accuracy: 0.356000

lr 2.100000e-07 reg 1.100000e+04 train accuracy: 0.382592 val accuracy: 0.413000

new best val
lr 2.100000e-07 reg 2.100000e+04 train accuracy: 0.362959 val accuracy: 0.377000

lr 2.100000e-07 reg 3.100000e+04 train accuracy: 0.345571 val accuracy: 0.374000

lr 2.100000e-07 reg 4.100000e+04 train accuracy: 0.344082 val accuracy: 0.376000

lr 2.100000e-07 reg 5.100000e+04 train accuracy: 0.345735 val accuracy: 0.361000

lr 2.100000e-07 reg 6.100000e+04 train accuracy: 0.343592 val accuracy: 0.357000

lr 2.100000e-07 reg 7.100000e+04 train accuracy: 0.338020 val accuracy: 0.350000

lr 2.100000e-07 reg 8.100000e+04 train accuracy: 0.333449 val accuracy: 0.323000

lr 2.100000e-07 reg 9.100000e+04 train accuracy: 0.324102 val accuracy: 0.336000

lr 3.100000e-07 reg 1.000000e+03 train accuracy: 0.375122 val accuracy: 0.379000

lr 3.100000e-07 reg 1.100000e+04 train accuracy: 0.364102 val accuracy: 0.380000

lr 3.100000e-07 reg 2.100000e+04 train accuracy: 0.357714 val accuracy: 0.366000

lr 3.100000e-07 reg 3.100000e+04 train accuracy: 0.347449 val accuracy: 0.353000

lr 3.100000e-07 reg 4.100000e+04 train accuracy: 0.340061 val accuracy: 0.359000

lr 3.100000e-07 reg 5.100000e+04 train accuracy: 0.345612 val accuracy: 0.356000

lr 3.100000e-07 reg 6.100000e+04 train accuracy: 0.321388 val accuracy: 0.336000

lr 3.100000e-07 reg 7.100000e+04 train accuracy: 0.337163 val accuracy: 0.336000

lr 3.100000e-07 reg 8.100000e+04 train accuracy: 0.326510 val accuracy: 0.346000

lr 3.100000e-07 reg 9.100000e+04 train accuracy: 0.321796 val accuracy: 0.331000

lr 4.100000e-07 reg 1.000000e+03 train accuracy: 0.392714 val accuracy: 0.374000

lr 4.100000e-07 reg 1.100000e+04 train accuracy: 0.361082 val accuracy: 0.375000

lr 4.100000e-07 reg 2.100000e+04 train accuracy: 0.343245 val accuracy: 0.325000

lr 4.100000e-07 reg 3.100000e+04 train accuracy: 0.344224 val accuracy: 0.353000

lr 4.100000e-07 reg 4.100000e+04 train accuracy: 0.316122 val accuracy: 0.332000

lr 4.100000e-07 reg 5.100000e+04 train accuracy: 0.337633 val accuracy: 0.349000

lr 4.100000e-07 reg 6.100000e+04 train accuracy: 0.319102 val accuracy: 0.330000

lr 4.100000e-07 reg 7.100000e+04 train accuracy: 0.324469 val accuracy: 0.338000

lr 4.100000e-07 reg 8.100000e+04 train accuracy: 0.285122 val accuracy: 0.291000

lr 4.100000e-07 reg 9.100000e+04 train accuracy: 0.298735 val accuracy: 0.306000

lr 5.100000e-07 reg 1.000000e+03 train accuracy: 0.366837 val accuracy: 0.377000

lr 5.100000e-07 reg 1.100000e+04 train accuracy: 0.352959 val accuracy: 0.376000

lr 5.100000e-07 reg 2.100000e+04 train accuracy: 0.340796 val accuracy: 0.355000

lr 5.100000e-07 reg 3.100000e+04 train accuracy: 0.322959 val accuracy: 0.342000

lr 5.100000e-07 reg 4.100000e+04 train accuracy: 0.297714 val accuracy: 0.326000

lr 5.100000e-07 reg 5.100000e+04 train accuracy: 0.323816 val accuracy: 0.335000

lr 5.100000e-07 reg 6.100000e+04 train accuracy: 0.328551 val accuracy: 0.341000

lr 5.100000e-07 reg 7.100000e+04 train accuracy: 0.299592 val accuracy: 0.306000

lr 5.100000e-07 reg 8.100000e+04 train accuracy: 0.307408 val accuracy: 0.311000

lr 5.100000e-07 reg 9.100000e+04 train accuracy: 0.300612 val accuracy: 0.288000

lr 6.100000e-07 reg 1.000000e+03 train accuracy: 0.359469 val accuracy: 0.351000

lr 6.100000e-07 reg 1.100000e+04 train accuracy: 0.349816 val accuracy: 0.362000

lr 6.100000e-07 reg 2.100000e+04 train accuracy: 0.314449 val accuracy: 0.323000

lr 6.100000e-07 reg 3.100000e+04 train accuracy: 0.312020 val accuracy: 0.323000

lr 6.100000e-07 reg 4.100000e+04 train accuracy: 0.307755 val accuracy: 0.319000

lr 6.100000e-07 reg 5.100000e+04 train accuracy: 0.289755 val accuracy: 0.289000

lr 6.100000e-07 reg 6.100000e+04 train accuracy: 0.311000 val accuracy: 0.316000

lr 6.100000e-07 reg 7.100000e+04 train accuracy: 0.308898 val accuracy: 0.334000

lr 6.100000e-07 reg 8.100000e+04 train accuracy: 0.276122 val accuracy: 0.281000

lr 6.100000e-07 reg 9.100000e+04 train accuracy: 0.276122 val accuracy: 0.283000

lr 7.100000e-07 reg 1.000000e+03 train accuracy: 0.369224 val accuracy: 0.348000

lr 7.100000e-07 reg 1.100000e+04 train accuracy: 0.320755 val accuracy: 0.314000

lr 7.100000e-07 reg 2.100000e+04 train accuracy: 0.320857 val accuracy: 0.312000

lr 7.100000e-07 reg 3.100000e+04 train accuracy: 0.301510 val accuracy: 0.308000

lr 7.100000e-07 reg 4.100000e+04 train accuracy: 0.316000 val accuracy: 0.323000

lr 7.100000e-07 reg 5.100000e+04 train accuracy: 0.300388 val accuracy: 0.313000

lr 7.100000e-07 reg 6.100000e+04 train accuracy: 0.301041 val accuracy: 0.305000

lr 7.100000e-07 reg 7.100000e+04 train accuracy: 0.289102 val accuracy: 0.303000

lr 7.100000e-07 reg 8.100000e+04 train accuracy: 0.241184 val accuracy: 0.239000

lr 7.100000e-07 reg 9.100000e+04 train accuracy: 0.268184 val accuracy: 0.281000

lr 8.100000e-07 reg 1.000000e+03 train accuracy: 0.383204 val accuracy: 0.372000

lr 8.100000e-07 reg 1.100000e+04 train accuracy: 0.335102 val accuracy: 0.350000

lr 8.100000e-07 reg 2.100000e+04 train accuracy: 0.316776 val accuracy: 0.328000

lr 8.100000e-07 reg 3.100000e+04 train accuracy: 0.315061 val accuracy: 0.319000

lr 8.100000e-07 reg 4.100000e+04 train accuracy: 0.322714 val accuracy: 0.349000

lr 8.100000e-07 reg 5.100000e+04 train accuracy: 0.255776 val accuracy: 0.248000

lr 8.100000e-07 reg 6.100000e+04 train accuracy: 0.258204 val accuracy: 0.289000

lr 8.100000e-07 reg 7.100000e+04 train accuracy: 0.263143 val accuracy: 0.258000

lr 8.100000e-07 reg 8.100000e+04 train accuracy: 0.280449 val accuracy: 0.265000

lr 8.100000e-07 reg 9.100000e+04 train accuracy: 0.239041 val accuracy: 0.247000

lr 9.100000e-07 reg 1.000000e+03 train accuracy: 0.359367 val accuracy: 0.353000

lr 9.100000e-07 reg 1.100000e+04 train accuracy: 0.333694 val accuracy: 0.336000

lr 9.100000e-07 reg 2.100000e+04 train accuracy: 0.311735 val accuracy: 0.313000

lr 9.100000e-07 reg 3.100000e+04 train accuracy: 0.276347 val accuracy: 0.283000

lr 9.100000e-07 reg 4.100000e+04 train accuracy: 0.286653 val accuracy: 0.293000

lr 9.100000e-07 reg 5.100000e+04 train accuracy: 0.288571 val accuracy: 0.299000

lr 9.100000e-07 reg 6.100000e+04 train accuracy: 0.254204 val accuracy: 0.267000

lr 9.100000e-07 reg 7.100000e+04 train accuracy: 0.250469 val accuracy: 0.235000

lr 9.100000e-07 reg 8.100000e+04 train accuracy: 0.257653 val accuracy: 0.267000

lr 9.100000e-07 reg 9.100000e+04 train accuracy: 0.244224 val accuracy: 0.234000

lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.230041 val accuracy: 0.234000
lr 1.000000e-08 reg 1.100000e+04 train accuracy: 0.241531 val accuracy: 0.268000
lr 1.000000e-08 reg 2.100000e+04 train accuracy: 0.252918 val accuracy: 0.249000
lr 1.000000e-08 reg 3.100000e+04 train accuracy: 0.268347 val accuracy: 0.266000
lr 1.000000e-08 reg 4.100000e+04 train accuracy: 0.284367 val accuracy: 0.296000
lr 1.000000e-08 reg 5.100000e+04 train accuracy: 0.299857 val accuracy: 0.308000
lr 1.000000e-08 reg 6.100000e+04 train accuracy: 0.321286 val accuracy: 0.324000
lr 1.000000e-08 reg 7.100000e+04 train accuracy: 0.329408 val accuracy: 0.324000
lr 1.000000e-08 reg 8.100000e+04 train accuracy: 0.335612 val accuracy: 0.360000
lr 1.000000e-08 reg 9.100000e+04 train accuracy: 0.340816 val accuracy: 0.357000
lr 1.100000e-07 reg 1.000000e+03 train accuracy: 0.323408 val accuracy: 0.329000
lr 1.100000e-07 reg 1.100000e+04 train accuracy: 0.383551 val accuracy: 0.396000
lr 1.100000e-07 reg 2.100000e+04 train accuracy: 0.369857 val accuracy: 0.375000
lr 1.100000e-07 reg 3.100000e+04 train accuracy: 0.356306 val accuracy: 0.358000
lr 1.100000e-07 reg 4.100000e+04 train accuracy: 0.360061 val accuracy: 0.359000
lr 1.100000e-07 reg 5.100000e+04 train accuracy: 0.347918 val accuracy: 0.353000
lr 1.100000e-07 reg 6.100000e+04 train accuracy: 0.353694 val accuracy: 0.357000
lr 1.100000e-07 reg 7.100000e+04 train accuracy: 0.344041 val accuracy: 0.359000
lr 1.100000e-07 reg 8.100000e+04 train accuracy: 0.346776 val accuracy: 0.358000
lr 1.100000e-07 reg 9.100000e+04 train accuracy: 0.344837 val accuracy: 0.354000
lr 2.100000e-07 reg 1.000000e+03 train accuracy: 0.357327 val accuracy: 0.356000
lr 2.100000e-07 reg 1.100000e+04 train accuracy: 0.382592 val accuracy: 0.413000
lr 2.100000e-07 reg 2.100000e+04 train accuracy: 0.362959 val accuracy: 0.377000
lr 2.100000e-07 reg 3.100000e+04 train accuracy: 0.345571 val accuracy: 0.374000
lr 2.100000e-07 reg 4.100000e+04 train accuracy: 0.344082 val accuracy: 0.376000
lr 2.100000e-07 reg 5.100000e+04 train accuracy: 0.345735 val accuracy: 0.361000
lr 2.100000e-07 reg 6.100000e+04 train accuracy: 0.343592 val accuracy: 0.357000
lr 2.100000e-07 reg 7.100000e+04 train accuracy: 0.338020 val accuracy: 0.350000
lr 2.100000e-07 reg 8.100000e+04 train accuracy: 0.333449 val accuracy: 0.323000
lr 2.100000e-07 reg 9.100000e+04 train accuracy: 0.324102 val accuracy: 0.336000
lr 3.100000e-07 reg 1.000000e+03 train accuracy: 0.375122 val accuracy: 0.379000
lr 3.100000e-07 reg 1.100000e+04 train accuracy: 0.364102 val accuracy: 0.380000
lr 3.100000e-07 reg 2.100000e+04 train accuracy: 0.357714 val accuracy: 0.366000
lr 3.100000e-07 reg 3.100000e+04 train accuracy: 0.347449 val accuracy: 0.353000
lr 3.100000e-07 reg 4.100000e+04 train accuracy: 0.340061 val accuracy: 0.359000
lr 3.100000e-07 reg 5.100000e+04 train accuracy: 0.345612 val accuracy: 0.356000
lr 3.100000e-07 reg 6.100000e+04 train accuracy: 0.321388 val accuracy: 0.336000
lr 3.100000e-07 reg 7.100000e+04 train accuracy: 0.337163 val accuracy: 0.336000
lr 3.100000e-07 reg 8.100000e+04 train accuracy: 0.326510 val accuracy: 0.346000
lr 3.100000e-07 reg 9.100000e+04 train accuracy: 0.321796 val accuracy: 0.331000
lr 4.100000e-07 reg 1.000000e+03 train accuracy: 0.392714 val accuracy: 0.374000
lr 4.100000e-07 reg 1.100000e+04 train accuracy: 0.361082 val accuracy: 0.375000
lr 4.100000e-07 reg 2.100000e+04 train accuracy: 0.343245 val accuracy: 0.325000
lr 4.100000e-07 reg 3.100000e+04 train accuracy: 0.344224 val accuracy: 0.353000
lr 4.100000e-07 reg 4.100000e+04 train accuracy: 0.316122 val accuracy: 0.332000
lr 4.100000e-07 reg 5.100000e+04 train accuracy: 0.337633 val accuracy: 0.349000
lr 4.100000e-07 reg 6.100000e+04 train accuracy: 0.319102 val accuracy: 0.330000
lr 4.100000e-07 reg 7.100000e+04 train accuracy: 0.324469 val accuracy: 0.338000
lr 4.100000e-07 reg 8.100000e+04 train accuracy: 0.285122 val accuracy: 0.291000
lr 4.100000e-07 reg 9.100000e+04 train accuracy: 0.298735 val accuracy: 0.306000
lr 5.100000e-07 reg 1.000000e+03 train accuracy: 0.366837 val accuracy: 0.377000
lr 5.100000e-07 reg 1.100000e+04 train accuracy: 0.352959 val accuracy: 0.376000
lr 5.100000e-07 reg 2.100000e+04 train accuracy: 0.340796 val accuracy: 0.355000
lr 5.100000e-07 reg 3.100000e+04 train accuracy: 0.322959 val accuracy: 0.342000
lr 5.100000e-07 reg 4.100000e+04 train accuracy: 0.297714 val accuracy: 0.326000
lr 5.100000e-07 reg 5.100000e+04 train accuracy: 0.323816 val accuracy: 0.335000
lr 5.100000e-07 reg 6.100000e+04 train accuracy: 0.328551 val accuracy: 0.341000
lr 5.100000e-07 reg 7.100000e+04 train accuracy: 0.299592 val accuracy: 0.306000
lr 5.100000e-07 reg 8.100000e+04 train accuracy: 0.307408 val accuracy: 0.311000
lr 5.100000e-07 reg 9.100000e+04 train accuracy: 0.300612 val accuracy: 0.288000
lr 6.100000e-07 reg 1.000000e+03 train accuracy: 0.359469 val accuracy: 0.351000
lr 6.100000e-07 reg 1.100000e+04 train accuracy: 0.349816 val accuracy: 0.362000
lr 6.100000e-07 reg 2.100000e+04 train accuracy: 0.314449 val accuracy: 0.323000
lr 6.100000e-07 reg 3.100000e+04 train accuracy: 0.312020 val accuracy: 0.323000
lr 6.100000e-07 reg 4.100000e+04 train accuracy: 0.307755 val accuracy: 0.319000
lr 6.100000e-07 reg 5.100000e+04 train accuracy: 0.289755 val accuracy: 0.289000
lr 6.100000e-07 reg 6.100000e+04 train accuracy: 0.311000 val accuracy: 0.316000
lr 6.100000e-07 reg 7.100000e+04 train accuracy: 0.308898 val accuracy: 0.334000
lr 6.100000e-07 reg 8.100000e+04 train accuracy: 0.276122 val accuracy: 0.281000
lr 6.100000e-07 reg 9.100000e+04 train accuracy: 0.276122 val accuracy: 0.283000
lr 7.100000e-07 reg 1.000000e+03 train accuracy: 0.369224 val accuracy: 0.348000
lr 7.100000e-07 reg 1.100000e+04 train accuracy: 0.320755 val accuracy: 0.314000
lr 7.100000e-07 reg 2.100000e+04 train accuracy: 0.320857 val accuracy: 0.312000
lr 7.100000e-07 reg 3.100000e+04 train accuracy: 0.301510 val accuracy: 0.308000
lr 7.100000e-07 reg 4.100000e+04 train accuracy: 0.316000 val accuracy: 0.323000
lr 7.100000e-07 reg 5.100000e+04 train accuracy: 0.300388 val accuracy: 0.313000
lr 7.100000e-07 reg 6.100000e+04 train accuracy: 0.301041 val accuracy: 0.305000
lr 7.100000e-07 reg 7.100000e+04 train accuracy: 0.289102 val accuracy: 0.303000
lr 7.100000e-07 reg 8.100000e+04 train accuracy: 0.241184 val accuracy: 0.239000
lr 7.100000e-07 reg 9.100000e+04 train accuracy: 0.268184 val accuracy: 0.281000
lr 8.100000e-07 reg 1.000000e+03 train accuracy: 0.383204 val accuracy: 0.372000
lr 8.100000e-07 reg 1.100000e+04 train accuracy: 0.335102 val accuracy: 0.350000
lr 8.100000e-07 reg 2.100000e+04 train accuracy: 0.316776 val accuracy: 0.328000
lr 8.100000e-07 reg 3.100000e+04 train accuracy: 0.315061 val accuracy: 0.319000
lr 8.100000e-07 reg 4.100000e+04 train accuracy: 0.322714 val accuracy: 0.349000
lr 8.100000e-07 reg 5.100000e+04 train accuracy: 0.255776 val accuracy: 0.248000
lr 8.100000e-07 reg 6.100000e+04 train accuracy: 0.258204 val accuracy: 0.289000
lr 8.100000e-07 reg 7.100000e+04 train accuracy: 0.263143 val accuracy: 0.258000
lr 8.100000e-07 reg 8.100000e+04 train accuracy: 0.280449 val accuracy: 0.265000
lr 8.100000e-07 reg 9.100000e+04 train accuracy: 0.239041 val accuracy: 0.247000
lr 9.100000e-07 reg 1.000000e+03 train accuracy: 0.359367 val accuracy: 0.353000
lr 9.100000e-07 reg 1.100000e+04 train accuracy: 0.333694 val accuracy: 0.336000
lr 9.100000e-07 reg 2.100000e+04 train accuracy: 0.311735 val accuracy: 0.313000
lr 9.100000e-07 reg 3.100000e+04 train accuracy: 0.276347 val accuracy: 0.283000
lr 9.100000e-07 reg 4.100000e+04 train accuracy: 0.286653 val accuracy: 0.293000
lr 9.100000e-07 reg 5.100000e+04 train accuracy: 0.288571 val accuracy: 0.299000
lr 9.100000e-07 reg 6.100000e+04 train accuracy: 0.254204 val accuracy: 0.267000
lr 9.100000e-07 reg 7.100000e+04 train accuracy: 0.250469 val accuracy: 0.235000
lr 9.100000e-07 reg 8.100000e+04 train accuracy: 0.257653 val accuracy: 0.267000
lr 9.100000e-07 reg 9.100000e+04 train accuracy: 0.244224 val accuracy: 0.234000
best validation accuracy achieved during cross-validation: 0.413000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Visualize the cross\PYZhy{}validation results}
\PY{k+kn}{import} \PY{n+nn}{math}
\PY{k+kn}{import} \PY{n+nn}{pdb}

\PY{c+c1}{\PYZsh{} pdb.set\PYZus{}trace()}

\PY{n}{x\PYZus{}scatter} \PY{o}{=} \PY{p}{[}\PY{n}{math}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{results}\PY{p}{]}
\PY{n}{y\PYZus{}scatter} \PY{o}{=} \PY{p}{[}\PY{n}{math}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{results}\PY{p}{]}

\PY{c+c1}{\PYZsh{} plot training accuracy}
\PY{n}{marker\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{100}
\PY{n}{colors} \PY{o}{=} \PY{p}{[}\PY{n}{results}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{results}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{n}{pad}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}scatter}\PY{p}{,} \PY{n}{y\PYZus{}scatter}\PY{p}{,} \PY{n}{marker\PYZus{}size}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{colors}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{coolwarm}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log learning rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log regularization strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CIFAR\PYZhy{}10 training accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} plot validation accuracy}
\PY{n}{colors} \PY{o}{=} \PY{p}{[}\PY{n}{results}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{results}\PY{p}{]} \PY{c+c1}{\PYZsh{} default size of markers is 20}
\PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}scatter}\PY{p}{,} \PY{n}{y\PYZus{}scatter}\PY{p}{,} \PY{n}{marker\PYZus{}size}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{colors}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{cm}\PY{o}{.}\PY{n}{coolwarm}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{colorbar}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log learning rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log regularization strength}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CIFAR\PYZhy{}10 validation accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{svm_files/svm_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Evaluate the best svm on test set}
\PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{test\PYZus{}accuracy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}test} \PY{o}{==} \PY{n}{y\PYZus{}test\PYZus{}pred}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear SVM on raw pixels final test set accuracy: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}accuracy}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
linear SVM on raw pixels final test set accuracy: 0.376000
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Visualize the learned weights for each class.}
\PY{c+c1}{\PYZsh{} Depending on your choice of learning rate and regularization strength, these may}
\PY{c+c1}{\PYZsh{} or may not be nice to look at.}
\PY{n}{w} \PY{o}{=} \PY{n}{best\PYZus{}svm}\PY{o}{.}\PY{n}{W}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{:}\PY{p}{]} \PY{c+c1}{\PYZsh{} strip out the bias}
\PY{n}{w} \PY{o}{=} \PY{n}{w}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{w\PYZus{}min}\PY{p}{,} \PY{n}{w\PYZus{}max} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{w}\PY{p}{)}
\PY{n}{classes} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{plane}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{car}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bird}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{deer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{frog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{horse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ship}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{truck}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
    \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
      
    \PY{c+c1}{\PYZsh{} Rescale the weights to be between 0 and 255}
    \PY{n}{wimg} \PY{o}{=} \PY{l+m+mf}{255.0} \PY{o}{*} \PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{w\PYZus{}min}\PY{p}{)} \PY{o}{/} \PY{p}{(}\PY{n}{w\PYZus{}max} \PY{o}{\PYZhy{}} \PY{n}{w\PYZus{}min}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{wimg}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{classes}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{svm_files/svm_23_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Inline question 2}

Describe what your visualized SVM weights look like, and offer a brief
explanation for why they look the way they do.

\(\color{blue}{\textit Your Answer:}\)

看起来像每个类别自己的大体轮廓特征。因为loss和gradient的配合驱使weights向更能描述自己类别特征的方向发展。


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
